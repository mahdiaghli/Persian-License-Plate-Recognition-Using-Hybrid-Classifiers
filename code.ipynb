{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af5a884f",
   "metadata": {},
   "source": [
    "phase1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a98f649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 1_plate.png\n",
      "Extracted 10_plate.png\n",
      "Extracted 100_plate.png\n",
      "Extracted 101_plate.png\n",
      "Extracted 102_plate.png\n",
      "Extracted 103_plate.png\n",
      "Extracted 104_plate.png\n",
      "Extracted 105_plate.png\n",
      "Extracted 106_plate.png\n",
      "Extracted 107_plate.png\n",
      "Extracted 108_plate.png\n",
      "Extracted 109_plate.png\n",
      "Extracted 11_plate.png\n",
      "Extracted 110_plate.png\n",
      "Extracted 111_plate.png\n",
      "Extracted 112_plate.png\n",
      "Extracted 113_plate.png\n",
      "Extracted 114_plate.png\n",
      "Extracted 115_plate.png\n",
      "Extracted 116_plate.png\n",
      "Extracted 117_plate.png\n",
      "Extracted 118_plate.png\n",
      "Extracted 119_plate.png\n",
      "Extracted 12_plate.png\n",
      "Extracted 120_plate.png\n",
      "Extracted 121_plate.png\n",
      "Extracted 122_plate.png\n",
      "Extracted 123_plate.png\n",
      "Extracted 124_plate.png\n",
      "Extracted 125_plate.png\n",
      "Extracted 126_plate.png\n",
      "Extracted 127_plate.png\n",
      "Extracted 128_plate.png\n",
      "Extracted 129_plate.png\n",
      "Extracted 13_plate.png\n",
      "Extracted 130_plate.png\n",
      "Extracted 131_plate.png\n",
      "Extracted 132_plate.png\n",
      "Extracted 133_plate.png\n",
      "Extracted 134_plate.png\n",
      "Extracted 135_plate.png\n",
      "Extracted 136_plate.png\n",
      "Extracted 137_plate.png\n",
      "Extracted 138_plate.png\n",
      "Extracted 139_plate.png\n",
      "Extracted 14_plate.png\n",
      "Extracted 140_plate.png\n",
      "Extracted 141_plate.png\n",
      "Extracted 142_plate.png\n",
      "Extracted 143_plate.png\n",
      "Extracted 144_plate.png\n",
      "Extracted 145_plate.png\n",
      "Extracted 146_plate.png\n",
      "Extracted 147_plate.png\n",
      "Extracted 148_plate.png\n",
      "Extracted 149_plate.png\n",
      "Extracted 15_plate.png\n",
      "Extracted 150_plate.png\n",
      "Extracted 151_plate.png\n",
      "Extracted 152_plate.png\n",
      "Extracted 153_plate.png\n",
      "Extracted 154_plate.png\n",
      "Extracted 155_plate.png\n",
      "Extracted 156_plate.png\n",
      "Extracted 157_plate.png\n",
      "Extracted 158_plate.png\n",
      "Extracted 159_plate.png\n",
      "Extracted 16_plate.png\n",
      "Extracted 160_plate.png\n",
      "Extracted 161_plate.png\n",
      "Extracted 162_plate.png\n",
      "Extracted 163_plate.png\n",
      "Extracted 164_plate.png\n",
      "Extracted 165_plate.png\n",
      "Extracted 166_plate.png\n",
      "Extracted 167_plate.png\n",
      "Extracted 168_plate.png\n",
      "Extracted 169_plate.png\n",
      "Extracted 17_plate.png\n",
      "Extracted 170_plate.png\n",
      "Extracted 171_plate.png\n",
      "Extracted 172_plate.png\n",
      "Extracted 173_plate.png\n",
      "Extracted 174_plate.png\n",
      "Extracted 175_plate.png\n",
      "Extracted 176_plate.png\n",
      "Extracted 177_plate.png\n",
      "Extracted 178_plate.png\n",
      "Extracted 179_plate.png\n",
      "Extracted 18_plate.png\n",
      "Extracted 180_plate.png\n",
      "Extracted 181_plate.png\n",
      "Extracted 182_plate.png\n",
      "Extracted 183_plate.png\n",
      "Extracted 184_plate.png\n",
      "Extracted 185_plate.png\n",
      "Extracted 186_plate.png\n",
      "Extracted 187_plate.png\n",
      "Extracted 188_plate.png\n",
      "Extracted 189_plate.png\n",
      "Extracted 19_plate.png\n",
      "Extracted 190_plate.png\n",
      "Extracted 191_plate.png\n",
      "Extracted 192_plate.png\n",
      "Extracted 193_plate.png\n",
      "Extracted 194_plate.png\n",
      "Extracted 195_plate.png\n",
      "Extracted 196_plate.png\n",
      "Extracted 197_plate.png\n",
      "Extracted 198_plate.png\n",
      "Extracted 199_plate.png\n",
      "Extracted 2_plate.png\n",
      "Extracted 20_plate.png\n",
      "Extracted 200_plate.png\n",
      "Extracted 201_plate.png\n",
      "Extracted 202_plate.png\n",
      "Extracted 203_plate.png\n",
      "Extracted 204_plate.png\n",
      "Extracted 205_plate.png\n",
      "Extracted 206_plate.png\n",
      "Extracted 207_plate.png\n",
      "Extracted 208_plate.png\n",
      "Extracted 209_plate.png\n",
      "Extracted 21_plate.png\n",
      "Extracted 210_plate.png\n",
      "Extracted 211_plate.png\n",
      "Extracted 212_plate.png\n",
      "Extracted 213_plate.png\n",
      "Extracted 214_plate.png\n",
      "Extracted 215_plate.png\n",
      "Extracted 216_plate.png\n",
      "Extracted 217_plate.png\n",
      "Extracted 22_plate.png\n",
      "Extracted 23_plate.png\n",
      "Extracted 24_plate.png\n",
      "Extracted 25_plate.png\n",
      "Extracted 26_plate.png\n",
      "Extracted 27_plate.png\n",
      "Extracted 28_plate.png\n",
      "Extracted 29_plate.png\n",
      "Extracted 3_plate.png\n",
      "Extracted 30_plate.png\n",
      "Extracted 31_plate.png\n",
      "Extracted 32_plate.png\n",
      "Extracted 33_plate.png\n",
      "Extracted 34_plate.png\n",
      "Extracted 35_plate.png\n",
      "Extracted 36_plate.png\n",
      "Extracted 37_plate.png\n",
      "Extracted 38_plate.png\n",
      "Extracted 39_plate.png\n",
      "Extracted 4_plate.png\n",
      "Extracted 40_plate.png\n",
      "Extracted 41_plate.png\n",
      "Extracted 42_plate.png\n",
      "Extracted 43_plate.png\n",
      "Extracted 44_plate.png\n",
      "Extracted 45_plate.png\n",
      "Extracted 46_plate.png\n",
      "Extracted 47_plate.png\n",
      "Extracted 48_plate.png\n",
      "Extracted 49_plate.png\n",
      "Extracted 5_plate.png\n",
      "Extracted 50_plate.png\n",
      "Extracted 51_plate.png\n",
      "Extracted 52_plate.png\n",
      "Extracted 53_plate.png\n",
      "Extracted 54_plate.png\n",
      "Extracted 55_plate.png\n",
      "Extracted 56_plate.png\n",
      "Extracted 57_plate.png\n",
      "Extracted 58_plate.png\n",
      "Extracted 59_plate.png\n",
      "Extracted 6_plate.png\n",
      "Extracted 60_plate.png\n",
      "Extracted 61_plate.png\n",
      "Extracted 62_plate.png\n",
      "Extracted 63_plate.png\n",
      "Extracted 64_plate.png\n",
      "Extracted 65_plate.png\n",
      "Extracted 66_plate.png\n",
      "Extracted 67_plate.png\n",
      "Extracted 68_plate.png\n",
      "Extracted 69_plate.png\n",
      "Extracted 7_plate.png\n",
      "Extracted 70_plate.png\n",
      "Extracted 71_plate.png\n",
      "Extracted 72_plate.png\n",
      "Extracted 73_plate.png\n",
      "Extracted 74_plate.png\n",
      "Extracted 75_plate.png\n",
      "Extracted 76_plate.png\n",
      "Extracted 77_plate.png\n",
      "Extracted 78_plate.png\n",
      "Extracted 79_plate.png\n",
      "Extracted 8_plate.png\n",
      "Extracted 80_plate.png\n",
      "Extracted 81_plate.png\n",
      "Extracted 82_plate.png\n",
      "Extracted 83_plate.png\n",
      "Extracted 84_plate.png\n",
      "Extracted 85_plate.png\n",
      "Extracted 86_plate.png\n",
      "Extracted 87_plate.png\n",
      "Extracted 88_plate.png\n",
      "Extracted 89_plate.png\n",
      "Extracted 9_plate.png\n",
      "Extracted 90_plate.png\n",
      "Extracted 91_plate.png\n",
      "Extracted 92_plate.png\n",
      "Extracted 93_plate.png\n",
      "Extracted 94_plate.png\n",
      "Extracted 95_plate.png\n",
      "Extracted 96_plate.png\n",
      "Extracted 97_plate.png\n",
      "Extracted 98_plate.png\n",
      "Extracted 99_plate.png\n",
      "Phase 1 extraction complete.\n"
     ]
    }
   ],
   "source": [
    "# Phase 1: Load data and extract vehicle plates based on annotations\n",
    "# Matches each image with its same-named .xml annotation file, ignores <filename> inside XML\n",
    "# Dependencies: opencv-python, xml.etree.ElementTree, os\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Paths (update these to your directories)\n",
    "ANNOTATIONS_DIR = r\"Plates2/Vehicle Plates annotations/Vehicle Plates annotations\"      # folder containing .xml files\n",
    "IMAGES_DIR      = r\"Plates2/Vehicle Plates 1280x1280/Vehicle Plates 1280x1280\"  # folder containing original images\n",
    "OUTPUT_DIR      = r\"plates33\"   # where to save cropped plates\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Iterate through image files and process corresponding annotation\n",
    "for img_file in os.listdir(IMAGES_DIR):\n",
    "    if not img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "        continue\n",
    "    base_name, _ = os.path.splitext(img_file)\n",
    "\n",
    "    img_path = os.path.join(IMAGES_DIR, img_file)\n",
    "    xml_path = os.path.join(ANNOTATIONS_DIR, f\"{base_name}.xml\")\n",
    "    if not os.path.isfile(xml_path):\n",
    "        print(f\"Skipping {img_file}: no annotation {base_name}.xml found.\")\n",
    "        continue\n",
    "\n",
    "    # Load image\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        print(f\"Warning: could not read image {img_path}\")\n",
    "        continue\n",
    "    actual_h, actual_w = img.shape[:2]\n",
    "\n",
    "    # Parse XML annotation\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Get annotated image dimensions for scaling\n",
    "    size = root.find('size')\n",
    "    ann_w = float(size.findtext('width'))\n",
    "    ann_h = float(size.findtext('height'))\n",
    "    scale_x = actual_w / ann_w\n",
    "    scale_y = actual_h / ann_h\n",
    "\n",
    "    # Process one or multiple objects\n",
    "    for obj in root.findall('object'):\n",
    "        bbox = obj.find('bndbox')\n",
    "        xmin = float(bbox.findtext('xmin'))\n",
    "        ymin = float(bbox.findtext('ymin'))\n",
    "        xmax = float(bbox.findtext('xmax'))\n",
    "        ymax = float(bbox.findtext('ymax'))\n",
    "\n",
    "        # Scale to actual image coords\n",
    "        x1 = int(xmin * scale_x)\n",
    "        y1 = int(ymin * scale_y)\n",
    "        x2 = int(xmax * scale_x)\n",
    "        y2 = int(ymax * scale_y)\n",
    "\n",
    "        # Crop and save\n",
    "        plate = img[y1:y2, x1:x2]\n",
    "        out_name = f\"{base_name}_plate.png\"\n",
    "        cv2.imwrite(os.path.join(OUTPUT_DIR, out_name), plate)\n",
    "        print(f\"Extracted {out_name}\")\n",
    "\n",
    "print(\"Phase 1 extraction complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ea9a5e",
   "metadata": {},
   "source": [
    "phase2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008b871e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Paths\n",
    "PLATES_DIR = r\"plates33\"\n",
    "OUTPUT_DIR = r\"characters9\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "def preprocess_plate(plate_img):\n",
    "    gray = cv2.cvtColor(plate_img, cv2.COLOR_BGR2GRAY)\n",
    "    edges = cv2.Canny(gray, 50, 150, apertureSize=3)\n",
    "    lines = cv2.HoughLinesP(edges, 1, np.pi/180, threshold=100, minLineLength=100, maxLineGap=10)\n",
    "    if lines is not None:\n",
    "        angles = [np.degrees(np.arctan2(y2-y1, x2-x1)) for [[x1,y1,x2,y2]] in lines if x2!=x1]\n",
    "        if angles:\n",
    "            m = np.median(angles)\n",
    "            if abs(m) < 45:\n",
    "                h, w = gray.shape\n",
    "                M = cv2.getRotationMatrix2D((w//2, h//2), m, 1)\n",
    "                gray = cv2.warpAffine(gray, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n",
    "    blurred = cv2.GaussianBlur(gray, (5,5), 0)\n",
    "    _, binary = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    kern = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3,3))\n",
    "    # connect diacritics to base character\n",
    "    binary = cv2.morphologyEx(binary, cv2.MORPH_DILATE, kern, iterations=1)\n",
    "    binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kern, iterations=1)\n",
    "    binary = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kern, iterations=1)\n",
    "    return binary\n",
    "\n",
    "\n",
    "def extract_characters(binary_img, plate_name):\n",
    "    num, labels, stats, cents = cv2.connectedComponentsWithStats(binary_img, 8)\n",
    "    h, w = binary_img.shape\n",
    "    area_tot = h * w\n",
    "    comps = []\n",
    "    for i in range(1, num):\n",
    "        x, y, ww, hh, area = stats[i]\n",
    "        ar = ww / float(hh) if hh > 0 else 0\n",
    "        if area > area_tot * 0.001 and area < area_tot * 0.25 and 0.3 < ar < 3.5:\n",
    "            cy = cents[i][1]\n",
    "            comps.append((i, x, y, ww, hh, cy))\n",
    "    if not comps:\n",
    "        return []\n",
    "    # line alignment\n",
    "    ys = np.array([c[5] for c in comps])\n",
    "    med = np.median(ys)\n",
    "    tol = h * 0.15\n",
    "    main = [c for c in comps if abs(c[5] - med) <= tol]\n",
    "    main.sort(key=lambda c: c[1])  # left-to-right\n",
    "\n",
    "    plate_dir = os.path.join(OUTPUT_DIR, plate_name)\n",
    "    os.makedirs(plate_dir, exist_ok=True)\n",
    "    saved = []\n",
    "    for idx, (lid, x, y, ww, hh, cy) in enumerate(main):\n",
    "        # extract component mask\n",
    "        mask = (labels == lid).astype(np.uint8) * 255\n",
    "        roi = mask[y:y+hh, x:x+ww]\n",
    "        pad = 5\n",
    "        pad_img = cv2.copyMakeBorder(roi, pad, pad, pad, pad, cv2.BORDER_CONSTANT, value=0)\n",
    "        # resize preserving aspect\n",
    "        tgt = (28, 28)\n",
    "        h2, w2 = pad_img.shape\n",
    "        ar = w2 / float(h2)\n",
    "        if ar > 1:\n",
    "            new_w = tgt[0]\n",
    "            new_h = int(new_w / ar)\n",
    "        else:\n",
    "            new_h = tgt[1]\n",
    "            new_w = int(new_h * ar)\n",
    "        resized = cv2.resize(pad_img, (new_w, new_h))\n",
    "        # invert so character is black, background white\n",
    "        inv_char = 255 - resized\n",
    "        # place on white canvas\n",
    "        canvas = np.ones(tgt, dtype=np.uint8) * 255\n",
    "        xo = (tgt[0] - new_w) // 2\n",
    "        yo = (tgt[1] - new_h) // 2\n",
    "        canvas[yo:yo+new_h, xo:xo+new_w] = inv_char\n",
    "        # save\n",
    "        out_path = os.path.join(plate_dir, f\"char{idx}.png\")\n",
    "        cv2.imwrite(out_path, canvas)\n",
    "        saved.append(canvas)\n",
    "        print(f\"Extracted character {idx} from {plate_name}\")\n",
    "    return saved\n",
    "\n",
    "# Main loop\n",
    "for f in os.listdir(PLATES_DIR):\n",
    "    if not f.lower().endswith(('.png','.jpg','.jpeg')):\n",
    "        continue\n",
    "    img = cv2.imread(os.path.join(PLATES_DIR, f))\n",
    "    if img is None:\n",
    "        continue\n",
    "    bin_img = preprocess_plate(img)\n",
    "    extract_characters(bin_img, os.path.splitext(f)[0])\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e979c5",
   "metadata": {},
   "source": [
    "phase 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd98e2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All images preprocessed and saved to: alpha_processed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# --- Configuration ---\n",
    "INPUT_DIR   = r\"alpha\"             # original folder\n",
    "OUTPUT_DIR  = r\"alpha_processed\"   # output folder with same subfolder structure\n",
    "TARGET_SIZE = (28, 28)             # desired output image size\n",
    "\n",
    "def preprocess_preserve_dark(img, tgt_size=TARGET_SIZE):\n",
    "    # 1) Binarize: black glyph → white, background stays black\n",
    "    _, bw = cv2.threshold(img, 250, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    # 2) Find all external contours (body + dots)\n",
    "    contours, _ = cv2.findContours(bw, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if contours:\n",
    "        # Compute a bounding box that covers every contour\n",
    "        x_min, y_min = img.shape[1], img.shape[0]\n",
    "        x_max, y_max = 0, 0\n",
    "        for cnt in contours:\n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "            x_min = min(x_min, x)\n",
    "            y_min = min(y_min, y)\n",
    "            x_max = max(x_max, x + w)\n",
    "            y_max = max(y_max, y + h)\n",
    "        cropped = img[y_min:y_max, x_min:x_max]\n",
    "    else:\n",
    "        cropped = img.copy()\n",
    "\n",
    "    # 3) Resize to fit tgt_size, preserving aspect ratio\n",
    "    h, w = cropped.shape\n",
    "    scale = min(tgt_size[0] / w, tgt_size[1] / h)\n",
    "    new_w, new_h = int(w * scale), int(h * scale)\n",
    "    resized = cv2.resize(cropped, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # 4) Center on white canvas\n",
    "    canvas = np.ones(tgt_size, dtype=np.uint8) * 255\n",
    "    x_off = (tgt_size[0] - new_w) // 2\n",
    "    y_off = (tgt_size[1] - new_h) // 2\n",
    "    canvas[y_off:y_off+new_h, x_off:x_off+new_w] = resized\n",
    "\n",
    "    return canvas\n",
    "\n",
    "# --- Prepare output folders ---\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "for label in os.listdir(INPUT_DIR):\n",
    "    src_folder = os.path.join(INPUT_DIR, label)\n",
    "    dst_folder = os.path.join(OUTPUT_DIR, label)\n",
    "    if not os.path.isdir(src_folder):\n",
    "        continue\n",
    "    os.makedirs(dst_folder, exist_ok=True)\n",
    "\n",
    "    # Process each image in this label-folder\n",
    "    for fname in os.listdir(src_folder):\n",
    "        if not fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            continue\n",
    "\n",
    "        in_path  = os.path.join(src_folder, fname)\n",
    "        out_path = os.path.join(dst_folder, fname)\n",
    "\n",
    "        img = cv2.imread(in_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None:\n",
    "            print(f\"⚠️  Could not read {in_path}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        processed = preprocess_preserve_dark(img)\n",
    "        cv2.imwrite(out_path, processed)\n",
    "\n",
    "print(\"✅ All images preprocessed and saved to:\", OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fc50c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading flattened data…\n",
      "Samples: 4299, feature dim: 784\n",
      "\n",
      "Training DecisionTree…\n",
      "Fitting 10 folds for each of 27 candidates, totalling 270 fits\n",
      "DecisionTree best params: {'clf__max_depth': 20, 'clf__min_samples_leaf': 1, 'pca__n_components': 0.95}\n",
      "\n",
      "Training SVM…\n",
      "Fitting 10 folds for each of 18 candidates, totalling 180 fits\n",
      "SVM best params: {'clf__C': 10, 'clf__gamma': 'scale', 'pca__n_components': 0.95}\n",
      "\n",
      "Training RandomForest…\n",
      "Fitting 10 folds for each of 27 candidates, totalling 270 fits\n",
      "RandomForest best params: {'clf__max_depth': None, 'clf__min_samples_leaf': 1, 'pca__n_components': 0.95}\n",
      "\n",
      "DecisionTree Accuracy: 0.7581\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      1-alef       0.73      0.95      0.83        20\n",
      "        10-d       0.56      0.45      0.50        20\n",
      "      11-zal       0.69      0.55      0.61        20\n",
      "        12-r       0.60      0.60      0.60        20\n",
      "        13-z       0.61      0.70      0.65        20\n",
      "       14-zh       0.72      0.65      0.68        20\n",
      "      15-sin       0.78      0.70      0.74        20\n",
      "     16-shin       0.79      0.75      0.77        20\n",
      "      17-sad       0.84      0.80      0.82        20\n",
      "      18-zad       0.79      0.55      0.65        20\n",
      "   19-t-long       0.83      0.75      0.79        20\n",
      "         2-b       0.73      0.80      0.76        20\n",
      "   20-z-long       0.59      0.65      0.62        20\n",
      "     21-ayin       0.46      0.60      0.52        20\n",
      "    22-ghyin       0.83      0.75      0.79        20\n",
      "        23-f       0.56      0.75      0.64        20\n",
      "      24-ghe       0.71      0.75      0.73        20\n",
      "        25-k       0.63      0.60      0.62        20\n",
      "        26-g       0.68      0.65      0.67        20\n",
      "       27-le       0.84      0.80      0.82        20\n",
      "        28-m       0.82      0.70      0.76        20\n",
      "        29-n       0.61      0.55      0.58        20\n",
      "         3-p       0.61      0.55      0.58        20\n",
      "        30-v       0.68      0.65      0.67        20\n",
      "       31-he       1.00      1.00      1.00        20\n",
      "       32-ye       0.78      0.90      0.84        20\n",
      "     33-zero       0.95      1.00      0.98        20\n",
      "      34-one       1.00      1.00      1.00        20\n",
      "      35-two       0.91      1.00      0.95        20\n",
      "    36-three       1.00      1.00      1.00        20\n",
      "     37-four       1.00      1.00      1.00        20\n",
      "     38-five       1.00      1.00      1.00        20\n",
      "      39-six       1.00      1.00      1.00        20\n",
      "         4-t       0.56      0.45      0.50        20\n",
      "    40-seven       1.00      1.00      1.00        20\n",
      "    41-eight       0.95      1.00      0.98        20\n",
      "     42-nine       1.00      1.00      1.00        20\n",
      " 43-anewfive       0.95      1.00      0.98        20\n",
      "        5-se       0.57      0.60      0.59        20\n",
      "       6-jim       0.62      0.75      0.68        20\n",
      "       7-che       0.42      0.40      0.41        20\n",
      "         8-h       0.65      0.75      0.70        20\n",
      "        9-kh       0.59      0.50      0.54        20\n",
      "\n",
      "    accuracy                           0.76       860\n",
      "   macro avg       0.76      0.76      0.76       860\n",
      "weighted avg       0.76      0.76      0.76       860\n",
      "\n",
      "\n",
      "SVM Accuracy: 0.8942\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      1-alef       1.00      0.95      0.97        20\n",
      "        10-d       0.94      0.85      0.89        20\n",
      "      11-zal       1.00      0.75      0.86        20\n",
      "        12-r       0.84      0.80      0.82        20\n",
      "        13-z       0.75      0.90      0.82        20\n",
      "       14-zh       0.81      0.85      0.83        20\n",
      "      15-sin       1.00      0.95      0.97        20\n",
      "     16-shin       0.89      0.80      0.84        20\n",
      "      17-sad       0.95      0.95      0.95        20\n",
      "      18-zad       0.95      0.95      0.95        20\n",
      "   19-t-long       0.82      0.90      0.86        20\n",
      "         2-b       0.75      0.90      0.82        20\n",
      "   20-z-long       0.80      0.80      0.80        20\n",
      "     21-ayin       0.95      0.90      0.92        20\n",
      "    22-ghyin       0.89      0.85      0.87        20\n",
      "        23-f       0.95      0.90      0.92        20\n",
      "      24-ghe       1.00      0.95      0.97        20\n",
      "        25-k       0.94      0.75      0.83        20\n",
      "        26-g       0.84      0.80      0.82        20\n",
      "       27-le       0.95      0.95      0.95        20\n",
      "        28-m       0.94      0.85      0.89        20\n",
      "        29-n       1.00      0.90      0.95        20\n",
      "         3-p       0.50      0.90      0.64        20\n",
      "        30-v       0.94      0.80      0.86        20\n",
      "       31-he       1.00      1.00      1.00        20\n",
      "       32-ye       1.00      1.00      1.00        20\n",
      "     33-zero       1.00      1.00      1.00        20\n",
      "      34-one       1.00      1.00      1.00        20\n",
      "      35-two       1.00      1.00      1.00        20\n",
      "    36-three       1.00      1.00      1.00        20\n",
      "     37-four       1.00      1.00      1.00        20\n",
      "     38-five       1.00      1.00      1.00        20\n",
      "      39-six       1.00      1.00      1.00        20\n",
      "         4-t       0.86      0.60      0.71        20\n",
      "    40-seven       1.00      1.00      1.00        20\n",
      "    41-eight       1.00      1.00      1.00        20\n",
      "     42-nine       1.00      1.00      1.00        20\n",
      " 43-anewfive       1.00      1.00      1.00        20\n",
      "        5-se       0.74      0.85      0.79        20\n",
      "       6-jim       0.60      0.90      0.72        20\n",
      "       7-che       0.67      0.60      0.63        20\n",
      "         8-h       0.94      0.80      0.86        20\n",
      "        9-kh       0.84      0.80      0.82        20\n",
      "\n",
      "    accuracy                           0.89       860\n",
      "   macro avg       0.91      0.89      0.90       860\n",
      "weighted avg       0.91      0.89      0.90       860\n",
      "\n",
      "\n",
      "RandomForest Accuracy: 0.8942\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      1-alef       0.90      0.95      0.93        20\n",
      "        10-d       0.95      0.95      0.95        20\n",
      "      11-zal       0.84      0.80      0.82        20\n",
      "        12-r       0.76      0.80      0.78        20\n",
      "        13-z       0.64      0.80      0.71        20\n",
      "       14-zh       0.84      0.80      0.82        20\n",
      "      15-sin       0.95      0.90      0.92        20\n",
      "     16-shin       0.80      0.80      0.80        20\n",
      "      17-sad       0.91      1.00      0.95        20\n",
      "      18-zad       0.95      0.95      0.95        20\n",
      "   19-t-long       0.78      0.90      0.84        20\n",
      "         2-b       0.87      1.00      0.93        20\n",
      "   20-z-long       0.78      0.70      0.74        20\n",
      "     21-ayin       0.82      0.90      0.86        20\n",
      "    22-ghyin       0.77      1.00      0.87        20\n",
      "        23-f       0.89      0.85      0.87        20\n",
      "      24-ghe       1.00      1.00      1.00        20\n",
      "        25-k       0.94      0.75      0.83        20\n",
      "        26-g       0.74      0.85      0.79        20\n",
      "       27-le       1.00      0.90      0.95        20\n",
      "        28-m       0.86      0.90      0.88        20\n",
      "        29-n       0.91      1.00      0.95        20\n",
      "         3-p       1.00      0.80      0.89        20\n",
      "        30-v       0.94      0.75      0.83        20\n",
      "       31-he       1.00      1.00      1.00        20\n",
      "       32-ye       1.00      1.00      1.00        20\n",
      "     33-zero       1.00      1.00      1.00        20\n",
      "      34-one       1.00      1.00      1.00        20\n",
      "      35-two       1.00      1.00      1.00        20\n",
      "    36-three       1.00      1.00      1.00        20\n",
      "     37-four       1.00      1.00      1.00        20\n",
      "     38-five       1.00      1.00      1.00        20\n",
      "      39-six       1.00      1.00      1.00        20\n",
      "         4-t       0.81      0.65      0.72        20\n",
      "    40-seven       1.00      1.00      1.00        20\n",
      "    41-eight       1.00      1.00      1.00        20\n",
      "     42-nine       1.00      1.00      1.00        20\n",
      " 43-anewfive       1.00      1.00      1.00        20\n",
      "        5-se       0.82      0.90      0.86        20\n",
      "       6-jim       0.76      0.80      0.78        20\n",
      "       7-che       0.82      0.45      0.58        20\n",
      "         8-h       0.71      0.85      0.77        20\n",
      "        9-kh       0.88      0.75      0.81        20\n",
      "\n",
      "    accuracy                           0.89       860\n",
      "   macro avg       0.90      0.89      0.89       860\n",
      "weighted avg       0.90      0.89      0.89       860\n",
      "\n",
      "Phase 3 (flat features) complete—all models trained on flattened 28×28 data.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Paths\n",
    "data_dir   = r\"alpha_processed\"  # preprocessed 28×28 images, one subfolder per label\n",
    "output_dir = r\"models_flat\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 1. Load flattened data with optional simple augmentation\n",
    "def load_data_flat(dir_path, augment=True):\n",
    "    X, y = [], []\n",
    "\n",
    "    def augment_image(img):\n",
    "        # you can add flips/translations if desired\n",
    "        return [img]\n",
    "\n",
    "    for label in sorted(os.listdir(dir_path)):\n",
    "        lblp = os.path.join(dir_path, label)\n",
    "        if not os.path.isdir(lblp):\n",
    "            continue\n",
    "\n",
    "        for fname in os.listdir(lblp):\n",
    "            if not fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                continue\n",
    "\n",
    "            path = os.path.join(lblp, fname)\n",
    "            img  = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "            if img is None or img.shape != (28,28):\n",
    "                continue\n",
    "\n",
    "            variants = augment_image(img) if augment else [img]\n",
    "            for im in variants:\n",
    "                X.append(im.flatten())     # **flatten the 28×28 image**\n",
    "                y.append(label)\n",
    "\n",
    "    return np.array(X, dtype=np.float32), np.array(y)\n",
    "\n",
    "print(\"Loading flattened data…\")\n",
    "X, y = load_data_flat(data_dir, augment=False)\n",
    "print(f\"Samples: {len(y)}, feature dim: {X.shape[1]}\")  # should be 784\n",
    "\n",
    "# 2. Encode & split\n",
    "le    = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)\n",
    "joblib.dump(le, os.path.join(output_dir, 'label_encoder.pkl'))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_enc, test_size=0.2, stratify=y_enc, random_state=42\n",
    ")\n",
    "\n",
    "# 3–4. Build pipelines with PCA + classifiers\n",
    "models = {\n",
    "    'DecisionTree': (DecisionTreeClassifier(random_state=42),\n",
    "                     {'clf__max_depth': [10,20,None],\n",
    "                      'clf__min_samples_leaf': [1,2,5]}),\n",
    "    'SVM': (SVC(kernel='rbf', probability=True, random_state=42),\n",
    "            {'clf__C': [0.1,1,10],\n",
    "             'clf__gamma': ['scale','auto']}),\n",
    "    'RandomForest': (RandomForestClassifier(n_estimators=200, random_state=42),\n",
    "                     {'clf__max_depth': [10,20,None],\n",
    "                      'clf__min_samples_leaf': [1,2,5]})\n",
    "}\n",
    "\n",
    "results = {}\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=42)\n",
    "\n",
    "for name, (est, params) in models.items():\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA(whiten=True, random_state=42)),\n",
    "        ('clf', est)\n",
    "    ])\n",
    "    # allow PCA variance tuning\n",
    "    params['pca__n_components'] = [0.95, 0.98, 0.99]\n",
    "\n",
    "    grid = GridSearchCV(pipe, params, cv=cv, n_jobs=-1, verbose=1)\n",
    "    print(f\"\\nTraining {name}…\")\n",
    "    grid.fit(X_train, y_train)\n",
    "    results[name] = grid\n",
    "\n",
    "    # save best estimator\n",
    "    joblib.dump(grid.best_estimator_,\n",
    "                os.path.join(output_dir, f\"{name.lower()}.pkl\"))\n",
    "    print(f\"{name} best params:\", grid.best_params_)\n",
    "\n",
    "# 5. Evaluation\n",
    "for name, grid in results.items():\n",
    "    y_pred = grid.predict(X_test)\n",
    "    acc    = accuracy_score(y_test, y_pred)\n",
    "    print(f\"\\n{name} Accuracy: {acc:.4f}\")\n",
    "    print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "    plt.title(f\"{name} Confusion Matrix\")\n",
    "    plt.xlabel('Predicted'); plt.ylabel('True')\n",
    "    plt.savefig(os.path.join(output_dir, f\"heatmap_{name}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "print(\"Phase 3 (flat features) complete—all models trained on flattened 28×28 data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5618fcd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and augmenting data...\n",
      "Total samples: 12897, feature dim: 324\n",
      "Training DecisionTree...\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "DecisionTree best params: {'clf__max_depth': None, 'clf__min_samples_leaf': 1}\n",
      "Training SVM...\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "SVM best params: {'clf__C': 10, 'clf__gamma': 'scale'}\n",
      "DecisionTree Accuracy: 0.8643\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      1-alef       0.82      0.83      0.83        60\n",
      "        10-d       0.84      0.82      0.83        60\n",
      "      11-zal       0.86      0.80      0.83        60\n",
      "        12-r       0.96      0.88      0.92        60\n",
      "        13-z       0.78      0.78      0.78        60\n",
      "       14-zh       0.81      0.78      0.80        60\n",
      "      15-sin       0.72      0.78      0.75        60\n",
      "     16-shin       0.78      0.70      0.74        60\n",
      "      17-sad       0.71      0.85      0.77        60\n",
      "      18-zad       0.76      0.80      0.78        60\n",
      "   19-t-long       0.88      0.83      0.85        60\n",
      "         2-b       0.79      0.82      0.80        60\n",
      "   20-z-long       0.80      0.87      0.83        60\n",
      "     21-ayin       0.84      0.80      0.82        60\n",
      "    22-ghyin       0.90      0.87      0.88        60\n",
      "        23-f       0.91      0.82      0.86        60\n",
      "      24-ghe       0.82      0.90      0.86        60\n",
      "        25-k       0.84      0.72      0.77        60\n",
      "        26-g       0.80      0.80      0.80        60\n",
      "       27-le       0.97      0.93      0.95        60\n",
      "        28-m       0.95      0.95      0.95        60\n",
      "        29-n       0.78      0.88      0.83        60\n",
      "         3-p       0.80      0.67      0.73        60\n",
      "        30-v       0.81      0.85      0.83        60\n",
      "       31-he       0.97      1.00      0.98        60\n",
      "       32-ye       0.91      0.98      0.94        60\n",
      "     33-zero       0.98      1.00      0.99        60\n",
      "      34-one       1.00      1.00      1.00        60\n",
      "      35-two       1.00      1.00      1.00        60\n",
      "    36-three       0.98      1.00      0.99        60\n",
      "     37-four       1.00      1.00      1.00        60\n",
      "     38-five       1.00      1.00      1.00        60\n",
      "      39-six       1.00      1.00      1.00        60\n",
      "         4-t       0.72      0.78      0.75        60\n",
      "    40-seven       1.00      1.00      1.00        60\n",
      "    41-eight       1.00      1.00      1.00        60\n",
      "     42-nine       1.00      1.00      1.00        60\n",
      " 43-anewfive       1.00      1.00      1.00        60\n",
      "        5-se       0.73      0.62      0.67        60\n",
      "       6-jim       0.75      0.70      0.72        60\n",
      "       7-che       0.77      0.83      0.80        60\n",
      "         8-h       0.78      0.82      0.80        60\n",
      "        9-kh       0.70      0.70      0.70        60\n",
      "\n",
      "    accuracy                           0.86      2580\n",
      "   macro avg       0.87      0.86      0.86      2580\n",
      "weighted avg       0.87      0.86      0.86      2580\n",
      "\n",
      "SVM Accuracy: 0.9822\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      1-alef       1.00      1.00      1.00        60\n",
      "        10-d       0.97      1.00      0.98        60\n",
      "      11-zal       1.00      1.00      1.00        60\n",
      "        12-r       0.95      1.00      0.98        60\n",
      "        13-z       0.97      0.95      0.96        60\n",
      "       14-zh       0.98      0.97      0.97        60\n",
      "      15-sin       1.00      0.98      0.99        60\n",
      "     16-shin       1.00      1.00      1.00        60\n",
      "      17-sad       1.00      1.00      1.00        60\n",
      "      18-zad       1.00      1.00      1.00        60\n",
      "   19-t-long       1.00      0.98      0.99        60\n",
      "         2-b       0.89      0.93      0.91        60\n",
      "   20-z-long       0.98      1.00      0.99        60\n",
      "     21-ayin       1.00      0.98      0.99        60\n",
      "    22-ghyin       0.98      1.00      0.99        60\n",
      "        23-f       1.00      0.98      0.99        60\n",
      "      24-ghe       1.00      1.00      1.00        60\n",
      "        25-k       0.98      1.00      0.99        60\n",
      "        26-g       1.00      0.97      0.98        60\n",
      "       27-le       1.00      0.98      0.99        60\n",
      "        28-m       1.00      1.00      1.00        60\n",
      "        29-n       1.00      1.00      1.00        60\n",
      "         3-p       0.86      0.90      0.88        60\n",
      "        30-v       1.00      1.00      1.00        60\n",
      "       31-he       1.00      1.00      1.00        60\n",
      "       32-ye       1.00      1.00      1.00        60\n",
      "     33-zero       1.00      1.00      1.00        60\n",
      "      34-one       1.00      1.00      1.00        60\n",
      "      35-two       1.00      1.00      1.00        60\n",
      "    36-three       1.00      1.00      1.00        60\n",
      "     37-four       1.00      1.00      1.00        60\n",
      "     38-five       1.00      1.00      1.00        60\n",
      "      39-six       1.00      1.00      1.00        60\n",
      "         4-t       0.90      0.93      0.92        60\n",
      "    40-seven       1.00      1.00      1.00        60\n",
      "    41-eight       1.00      1.00      1.00        60\n",
      "     42-nine       1.00      1.00      1.00        60\n",
      " 43-anewfive       1.00      1.00      1.00        60\n",
      "        5-se       0.96      0.88      0.92        60\n",
      "       6-jim       0.90      0.90      0.90        60\n",
      "       7-che       0.95      0.92      0.93        60\n",
      "         8-h       0.97      0.97      0.97        60\n",
      "        9-kh       1.00      1.00      1.00        60\n",
      "\n",
      "    accuracy                           0.98      2580\n",
      "   macro avg       0.98      0.98      0.98      2580\n",
      "weighted avg       0.98      0.98      0.98      2580\n",
      "\n",
      "All done. Accuracies reported above.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Paths\n",
    "data_dir = r\"alpha_processed\"  # organized by label\n",
    "output_dir = r\"models_flat30\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 1. Load data with augmentation and resize to 28x28\n",
    "\n",
    "def load_data(dir_path, augment=True):\n",
    "    X, y = [], []\n",
    "    # target size for HOG and pixel features\n",
    "    win_size = (28, 28)\n",
    "    # HOG parameters matching 28x28\n",
    "    hog = cv2.HOGDescriptor(\n",
    "        _winSize=win_size,\n",
    "        _blockSize=(14,14),\n",
    "        _blockStride=(7,7),\n",
    "        _cellSize=(7,7),\n",
    "        _nbins=9\n",
    "    )\n",
    "    def augment_image(img):\n",
    "        rots = [0, -10, 10]\n",
    "        out = []\n",
    "        h, w = img.shape\n",
    "        center = (w//2, h//2)\n",
    "        for ang in rots:\n",
    "            M = cv2.getRotationMatrix2D(center, ang, 1.0)\n",
    "            rot = cv2.warpAffine(img, M, (w, h), flags=cv2.INTER_LINEAR,\n",
    "                                 borderMode=cv2.BORDER_CONSTANT, borderValue=255)\n",
    "            out.append(rot)\n",
    "        return out\n",
    "\n",
    "    for label in sorted(os.listdir(dir_path)):\n",
    "        lbl_path = os.path.join(dir_path, label)\n",
    "        if not os.path.isdir(lbl_path):\n",
    "            continue\n",
    "        for fname in os.listdir(lbl_path):\n",
    "            if not fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                continue\n",
    "            img = cv2.imread(os.path.join(lbl_path, fname), cv2.IMREAD_GRAYSCALE)\n",
    "            if img is None:\n",
    "                continue\n",
    "            # Resize to 28x28\n",
    "            img = cv2.resize(img, win_size)\n",
    "            imgs = augment_image(img) if augment else [img]\n",
    "            for im in imgs:\n",
    "                # extract HOG features\n",
    "                desc = hog.compute(im)\n",
    "                X.append(desc.flatten())\n",
    "                y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "print(\"Loading and augmenting data...\")\n",
    "X, y = load_data(data_dir)\n",
    "print(f\"Total samples: {len(y)}, feature dim: {X.shape[1]}\")\n",
    "\n",
    "# 2. Encode labels and split\n",
    "y_encoder = LabelEncoder()\n",
    "y_enc = y_encoder.fit_transform(y)\n",
    "joblib.dump(y_encoder, os.path.join(output_dir, 'label_encoder.pkl'))\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_enc, test_size=0.2, stratify=y_enc, random_state=42\n",
    ")\n",
    "\n",
    "# 3. PCA for dimensionality reduction\n",
    "pca = PCA(n_components=0.99, whiten=True, random_state=42)\n",
    "\n",
    "# 4. Define models and hyperparameters\n",
    "models = {\n",
    "    'DecisionTree': (DecisionTreeClassifier(random_state=42),\n",
    "                     {'clf__max_depth': [10, 20, None], 'clf__min_samples_leaf': [1, 2, 5]}),\n",
    "    'SVM': (SVC(kernel='rbf', probability=True, random_state=42),\n",
    "            {'clf__C': [0.1, 1, 10], 'clf__gamma': ['scale', 'auto']})\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, (estimator, params) in models.items():\n",
    "    pipe = Pipeline([('scaler', StandardScaler()), ('pca', pca), ('clf', estimator)])\n",
    "    grid = GridSearchCV(pipe, params, cv=5, n_jobs=-1, verbose=1)\n",
    "    print(f\"Training {name}...\")\n",
    "    grid.fit(X_train, y_train)\n",
    "    results[name] = grid\n",
    "    joblib.dump(grid.best_estimator_, os.path.join(output_dir, f\"{name.lower()}.pkl\"))\n",
    "    print(f\"{name} best params: {grid.best_params_}\")\n",
    "\n",
    "# 5. Evaluation and accuracy reporting\n",
    "for name, grid in results.items():\n",
    "    y_pred = grid.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{name} Accuracy: {acc:.4f}\")\n",
    "\n",
    "    # detailed report\n",
    "    print(classification_report(y_test, y_pred, target_names=y_encoder.classes_))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    np.savetxt(os.path.join(output_dir, f\"cm_{name}.csv\"), cm, fmt='%d', delimiter=',')\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=y_encoder.classes_, yticklabels=y_encoder.classes_)\n",
    "    plt.title(f\"{name} Confusion Matrix\")\n",
    "    plt.xlabel('Predicted'); plt.ylabel('True')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f\"heatmap_{name}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "print(\"All done. Accuracies reported above.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "689e9846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading & augmenting data…\n",
      "Samples: 25794, feature dim: 324\n",
      "Training DecisionTree...\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "DecisionTree best params: {'clf__max_depth': None, 'clf__min_samples_leaf': 1}\n",
      "Training SVM...\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "SVM best params: {'clf__C': 10, 'clf__gamma': 'scale'}\n",
      "\n",
      "DecisionTree Accuracy: 0.8946\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      1-alef       0.93      0.93      0.93       120\n",
      "        10-d       0.88      0.93      0.91       120\n",
      "      11-zal       0.89      0.91      0.90       120\n",
      "        12-r       0.89      0.82      0.86       120\n",
      "        13-z       0.75      0.76      0.75       120\n",
      "       14-zh       0.78      0.84      0.81       120\n",
      "      15-sin       0.82      0.90      0.86       120\n",
      "     16-shin       0.83      0.81      0.82       120\n",
      "      17-sad       0.86      0.87      0.86       120\n",
      "      18-zad       0.80      0.78      0.79       120\n",
      "   19-t-long       0.90      0.93      0.91       120\n",
      "         2-b       0.84      0.88      0.86       120\n",
      "   20-z-long       0.84      0.89      0.87       120\n",
      "     21-ayin       0.84      0.85      0.84       120\n",
      "    22-ghyin       0.88      0.88      0.88       120\n",
      "        23-f       0.96      0.91      0.93       120\n",
      "      24-ghe       0.92      0.89      0.91       120\n",
      "        25-k       0.82      0.78      0.80       120\n",
      "        26-g       0.87      0.88      0.87       120\n",
      "       27-le       0.99      0.96      0.97       120\n",
      "        28-m       0.93      0.93      0.93       120\n",
      "        29-n       0.88      0.93      0.91       120\n",
      "         3-p       0.86      0.80      0.83       120\n",
      "        30-v       0.92      0.88      0.90       120\n",
      "       31-he       1.00      1.00      1.00       120\n",
      "       32-ye       0.98      0.99      0.98       120\n",
      "     33-zero       1.00      0.99      1.00       120\n",
      "      34-one       1.00      0.98      0.99       120\n",
      "      35-two       1.00      1.00      1.00       120\n",
      "    36-three       1.00      0.98      0.99       120\n",
      "     37-four       0.98      1.00      0.99       120\n",
      "     38-five       1.00      1.00      1.00       120\n",
      "      39-six       0.99      1.00      1.00       120\n",
      "         4-t       0.82      0.78      0.80       119\n",
      "    40-seven       1.00      1.00      1.00       120\n",
      "    41-eight       0.99      1.00      1.00       120\n",
      "     42-nine       1.00      1.00      1.00       120\n",
      " 43-anewfive       1.00      0.99      1.00       120\n",
      "        5-se       0.74      0.77      0.75       120\n",
      "       6-jim       0.68      0.66      0.67       120\n",
      "       7-che       0.73      0.75      0.74       120\n",
      "         8-h       0.87      0.83      0.85       120\n",
      "        9-kh       0.83      0.80      0.82       120\n",
      "\n",
      "    accuracy                           0.89      5159\n",
      "   macro avg       0.90      0.89      0.89      5159\n",
      "weighted avg       0.90      0.89      0.89      5159\n",
      "\n",
      "\n",
      "SVM Accuracy: 0.9952\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      1-alef       1.00      1.00      1.00       120\n",
      "        10-d       1.00      1.00      1.00       120\n",
      "      11-zal       1.00      1.00      1.00       120\n",
      "        12-r       0.98      1.00      0.99       120\n",
      "        13-z       0.98      0.98      0.98       120\n",
      "       14-zh       1.00      0.98      0.99       120\n",
      "      15-sin       1.00      1.00      1.00       120\n",
      "     16-shin       1.00      1.00      1.00       120\n",
      "      17-sad       1.00      1.00      1.00       120\n",
      "      18-zad       1.00      1.00      1.00       120\n",
      "   19-t-long       1.00      1.00      1.00       120\n",
      "         2-b       0.97      0.98      0.98       120\n",
      "   20-z-long       1.00      1.00      1.00       120\n",
      "     21-ayin       1.00      0.98      0.99       120\n",
      "    22-ghyin       0.99      1.00      1.00       120\n",
      "        23-f       1.00      1.00      1.00       120\n",
      "      24-ghe       1.00      1.00      1.00       120\n",
      "        25-k       0.98      0.98      0.98       120\n",
      "        26-g       0.98      0.98      0.98       120\n",
      "       27-le       1.00      1.00      1.00       120\n",
      "        28-m       1.00      1.00      1.00       120\n",
      "        29-n       1.00      0.99      1.00       120\n",
      "         3-p       0.98      0.97      0.98       120\n",
      "        30-v       1.00      0.99      1.00       120\n",
      "       31-he       1.00      1.00      1.00       120\n",
      "       32-ye       1.00      1.00      1.00       120\n",
      "     33-zero       1.00      1.00      1.00       120\n",
      "      34-one       1.00      1.00      1.00       120\n",
      "      35-two       1.00      1.00      1.00       120\n",
      "    36-three       1.00      1.00      1.00       120\n",
      "     37-four       1.00      1.00      1.00       120\n",
      "     38-five       1.00      1.00      1.00       120\n",
      "      39-six       1.00      1.00      1.00       120\n",
      "         4-t       0.99      0.99      0.99       119\n",
      "    40-seven       1.00      1.00      1.00       120\n",
      "    41-eight       1.00      1.00      1.00       120\n",
      "     42-nine       1.00      1.00      1.00       120\n",
      " 43-anewfive       1.00      1.00      1.00       120\n",
      "        5-se       0.99      0.99      0.99       120\n",
      "       6-jim       0.96      0.99      0.98       120\n",
      "       7-che       0.99      0.97      0.98       120\n",
      "         8-h       0.99      0.99      0.99       120\n",
      "        9-kh       1.00      1.00      1.00       120\n",
      "\n",
      "    accuracy                           1.00      5159\n",
      "   macro avg       1.00      1.00      1.00      5159\n",
      "weighted avg       1.00      1.00      1.00      5159\n",
      "\n",
      "\n",
      "All models trained and evaluated.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Paths\n",
    "data_dir = r\"alpha_processed\"  # organized by label\n",
    "output_dir = r\"models_flat3\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Augmentation function\n",
    "def augment_image(img):\n",
    "    h, w = img.shape\n",
    "    out = []\n",
    "\n",
    "    # Rotation\n",
    "    center = (w // 2, h // 2)\n",
    "    for angle in [0, -10, 10]:\n",
    "        M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "        rotated = cv2.warpAffine(img, M, (w, h),\n",
    "                                 flags=cv2.INTER_LINEAR,\n",
    "                                 borderMode=cv2.BORDER_CONSTANT,\n",
    "                                 borderValue=255)\n",
    "        out.append(rotated)\n",
    "\n",
    "    # Zoom-out\n",
    "    scale = 0.9\n",
    "    nw, nh = int(w * scale), int(h * scale)\n",
    "    if nw > 0 and nh > 0:\n",
    "        resized = cv2.resize(img, (nw, nh))\n",
    "        canvas = np.ones((h, w), dtype=np.uint8) * 255\n",
    "        x0 = (w - nw) // 2\n",
    "        y0 = (h - nh) // 2\n",
    "        canvas[y0:y0 + nh, x0:x0 + nw] = resized\n",
    "        out.append(canvas)\n",
    "\n",
    "    # Gaussian noise\n",
    "    noise = np.random.normal(0, 10, img.shape).astype(np.int16)\n",
    "    noisy = np.clip(img.astype(np.int16) + noise, 0, 255).astype(np.uint8)\n",
    "    out.append(noisy)\n",
    "\n",
    "    # Contrast\n",
    "    contrast = cv2.convertScaleAbs(img, alpha=1.2, beta=0)\n",
    "    out.append(contrast)\n",
    "\n",
    "    return out\n",
    "\n",
    "# Data loader\n",
    "def load_data(dir_path, augment=True):\n",
    "    X, y = [], []\n",
    "    win_size = (28, 28)\n",
    "    hog = cv2.HOGDescriptor(\n",
    "        _winSize=win_size,\n",
    "        _blockSize=(14,14),\n",
    "        _blockStride=(7,7),\n",
    "        _cellSize=(7,7),\n",
    "        _nbins=9\n",
    "    )\n",
    "    for label in sorted(os.listdir(dir_path)):\n",
    "        lbl_path = os.path.join(dir_path, label)\n",
    "        if not os.path.isdir(lbl_path):\n",
    "            continue\n",
    "        for fname in os.listdir(lbl_path):\n",
    "            if not fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                continue\n",
    "            raw = cv2.imread(os.path.join(lbl_path, fname), cv2.IMREAD_GRAYSCALE)\n",
    "            if raw is None:\n",
    "                continue\n",
    "            img = cv2.resize(raw, win_size)\n",
    "            variants = augment_image(img) if augment else [img]\n",
    "            for im in variants:\n",
    "                X.append(hog.compute(im).flatten())\n",
    "                y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load data\n",
    "print(\"Loading & augmenting data…\")\n",
    "X, y = load_data(data_dir, augment=True)\n",
    "print(f\"Samples: {len(y)}, feature dim: {X.shape[1]}\")\n",
    "\n",
    "# Encode labels and split\n",
    "y_encoder = LabelEncoder()\n",
    "y_enc = y_encoder.fit_transform(y)\n",
    "joblib.dump(y_encoder, os.path.join(output_dir, 'label_encoder.pkl'))\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_enc, test_size=0.2, stratify=y_enc, random_state=42\n",
    ")\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=0.99, whiten=True, random_state=42)\n",
    "\n",
    "# Models and hyperparameters\n",
    "models = {\n",
    "    'DecisionTree': (DecisionTreeClassifier(random_state=42),\n",
    "                     {'clf__max_depth': [10, 20, None],\n",
    "                      'clf__min_samples_leaf': [1, 2, 5]}),\n",
    "    'SVM': (SVC(kernel='rbf', probability=True, random_state=42),\n",
    "            {'clf__C': [0.1, 1, 10],\n",
    "             'clf__gamma': ['scale', 'auto']})\n",
    "}\n",
    "\n",
    "# Train and tune\n",
    "results = {}\n",
    "for name, (estimator, params) in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', pca),\n",
    "        ('clf', estimator)\n",
    "    ])\n",
    "    grid = GridSearchCV(pipe, params, cv=5, n_jobs=-1, verbose=1)\n",
    "    grid.fit(X_train, y_train)\n",
    "    results[name] = grid\n",
    "    joblib.dump(grid.best_estimator_, os.path.join(output_dir, f\"{name.lower()}.pkl\"))\n",
    "    print(f\"{name} best params: {grid.best_params_}\")\n",
    "\n",
    "# Evaluation\n",
    "for name, model in results.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"\\n{name} Accuracy: {acc:.4f}\")\n",
    "    print(classification_report(y_test, y_pred, target_names=y_encoder.classes_))\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    np.savetxt(os.path.join(output_dir, f\"cm_{name}.csv\"), cm, fmt='%d', delimiter=',')\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=y_encoder.classes_,\n",
    "                yticklabels=y_encoder.classes_)\n",
    "    plt.title(f\"{name} Confusion Matrix\")\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f\"heatmap_{name}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "print(\"\\nAll models trained and evaluated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3b3f29d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and augmenting data...\n",
      "Total samples: 21495, feature dim: 324\n",
      "Training DecisionTree...\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "DecisionTree best params: {'clf__criterion': 'entropy', 'clf__max_depth': 20, 'clf__min_samples_leaf': 1, 'pca__n_components': 0.95}\n",
      "Training SVM...\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "SVM best params: {'clf__C': 10, 'clf__gamma': 'scale', 'clf__kernel': 'rbf', 'pca__n_components': 0.95}\n",
      "DecisionTree Accuracy: 0.8930\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      1-alef       0.94      0.88      0.91       100\n",
      "        10-d       0.89      0.85      0.87       100\n",
      "      11-zal       0.89      0.90      0.90       100\n",
      "        12-r       0.84      0.93      0.88       100\n",
      "        13-z       0.77      0.67      0.72       100\n",
      "       14-zh       0.79      0.83      0.81       100\n",
      "      15-sin       0.85      0.85      0.85       100\n",
      "     16-shin       0.78      0.76      0.77       100\n",
      "      17-sad       0.86      0.83      0.84       100\n",
      "      18-zad       0.77      0.86      0.81       100\n",
      "   19-t-long       0.97      0.87      0.92       100\n",
      "         2-b       0.81      0.82      0.82       100\n",
      "   20-z-long       0.84      0.92      0.88       100\n",
      "     21-ayin       0.86      0.82      0.84       100\n",
      "    22-ghyin       0.81      0.94      0.87       100\n",
      "        23-f       0.91      0.91      0.91       100\n",
      "      24-ghe       0.86      0.88      0.87       100\n",
      "        25-k       0.81      0.78      0.80       100\n",
      "        26-g       0.80      0.85      0.83       100\n",
      "       27-le       0.95      0.91      0.93       100\n",
      "        28-m       0.98      0.96      0.97       100\n",
      "        29-n       0.86      0.83      0.85       100\n",
      "         3-p       0.82      0.80      0.81       100\n",
      "        30-v       0.94      0.98      0.96       100\n",
      "       31-he       0.97      1.00      0.99       100\n",
      "       32-ye       0.98      0.95      0.96       100\n",
      "     33-zero       0.98      1.00      0.99       100\n",
      "      34-one       0.99      1.00      1.00       100\n",
      "      35-two       1.00      1.00      1.00       100\n",
      "    36-three       1.00      1.00      1.00       100\n",
      "     37-four       0.98      1.00      0.99       100\n",
      "     38-five       1.00      1.00      1.00       100\n",
      "      39-six       0.97      1.00      0.99       100\n",
      "         4-t       0.83      0.80      0.81        99\n",
      "    40-seven       1.00      1.00      1.00       100\n",
      "    41-eight       1.00      1.00      1.00       100\n",
      "     42-nine       1.00      1.00      1.00       100\n",
      " 43-anewfive       0.99      1.00      1.00       100\n",
      "        5-se       0.77      0.81      0.79       100\n",
      "       6-jim       0.71      0.71      0.71       100\n",
      "       7-che       0.85      0.80      0.82       100\n",
      "         8-h       0.88      0.87      0.87       100\n",
      "        9-kh       0.91      0.83      0.87       100\n",
      "\n",
      "    accuracy                           0.89      4299\n",
      "   macro avg       0.89      0.89      0.89      4299\n",
      "weighted avg       0.89      0.89      0.89      4299\n",
      "\n",
      "SVM Accuracy: 0.9919\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      1-alef       1.00      1.00      1.00       100\n",
      "        10-d       1.00      1.00      1.00       100\n",
      "      11-zal       0.99      0.99      0.99       100\n",
      "        12-r       0.96      1.00      0.98       100\n",
      "        13-z       0.98      0.94      0.96       100\n",
      "       14-zh       0.98      0.98      0.98       100\n",
      "      15-sin       0.99      0.99      0.99       100\n",
      "     16-shin       1.00      0.98      0.99       100\n",
      "      17-sad       0.99      0.99      0.99       100\n",
      "      18-zad       0.99      1.00      1.00       100\n",
      "   19-t-long       1.00      1.00      1.00       100\n",
      "         2-b       0.94      0.98      0.96       100\n",
      "   20-z-long       1.00      1.00      1.00       100\n",
      "     21-ayin       1.00      0.99      0.99       100\n",
      "    22-ghyin       0.99      0.99      0.99       100\n",
      "        23-f       0.99      0.99      0.99       100\n",
      "      24-ghe       1.00      1.00      1.00       100\n",
      "        25-k       0.98      1.00      0.99       100\n",
      "        26-g       1.00      0.98      0.99       100\n",
      "       27-le       1.00      1.00      1.00       100\n",
      "        28-m       1.00      1.00      1.00       100\n",
      "        29-n       1.00      1.00      1.00       100\n",
      "         3-p       0.97      0.95      0.96       100\n",
      "        30-v       1.00      1.00      1.00       100\n",
      "       31-he       1.00      1.00      1.00       100\n",
      "       32-ye       1.00      1.00      1.00       100\n",
      "     33-zero       1.00      1.00      1.00       100\n",
      "      34-one       1.00      1.00      1.00       100\n",
      "      35-two       1.00      1.00      1.00       100\n",
      "    36-three       1.00      1.00      1.00       100\n",
      "     37-four       1.00      1.00      1.00       100\n",
      "     38-five       1.00      1.00      1.00       100\n",
      "      39-six       1.00      1.00      1.00       100\n",
      "         4-t       0.98      0.99      0.98        99\n",
      "    40-seven       1.00      1.00      1.00       100\n",
      "    41-eight       1.00      1.00      1.00       100\n",
      "     42-nine       1.00      1.00      1.00       100\n",
      " 43-anewfive       1.00      1.00      1.00       100\n",
      "        5-se       0.99      0.98      0.98       100\n",
      "       6-jim       0.95      0.99      0.97       100\n",
      "       7-che       1.00      0.95      0.97       100\n",
      "         8-h       0.99      1.00      1.00       100\n",
      "        9-kh       0.99      0.99      0.99       100\n",
      "\n",
      "    accuracy                           0.99      4299\n",
      "   macro avg       0.99      0.99      0.99      4299\n",
      "weighted avg       0.99      0.99      0.99      4299\n",
      "\n",
      "All done. Accuracies reported above.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Paths\n",
    "data_dir = r\"alpha_processed\"  # organized by label\n",
    "output_dir = r\"models_flat30\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 1. Load data with augmentation and resize to 28x28\n",
    "def load_data(dir_path, augment=True):\n",
    "    X, y = [], []\n",
    "    win_size = (28, 28)  # target size for HOG and pixel features\n",
    "    hog = cv2.HOGDescriptor(\n",
    "        _winSize=win_size,\n",
    "        _blockSize=(14, 14),\n",
    "        _blockStride=(7, 7),\n",
    "        _cellSize=(7, 7),\n",
    "        _nbins=9\n",
    "    )\n",
    "    \n",
    "    def augment_image(img):\n",
    "        rots = [0, -10, 10]  # rotations in degrees\n",
    "        trans = [(-2, 0), (2, 0)]  # horizontal translations: left and right by 2 pixels\n",
    "        out = []\n",
    "        h, w = img.shape\n",
    "        center = (w // 2, h // 2)\n",
    "        \n",
    "        # Rotations\n",
    "        for ang in rots:\n",
    "            M_rot = cv2.getRotationMatrix2D(center, ang, 1.0)\n",
    "            rot_img = cv2.warpAffine(img, M_rot, (w, h), flags=cv2.INTER_LINEAR,\n",
    "                                     borderMode=cv2.BORDER_CONSTANT, borderValue=255)\n",
    "            out.append(rot_img)\n",
    "        \n",
    "        # Translations on original image\n",
    "        for dx, dy in trans:\n",
    "            M_trans = np.float32([[1, 0, dx], [0, 1, dy]])\n",
    "            trans_img = cv2.warpAffine(img, M_trans, (w, h), flags=cv2.INTER_LINEAR,\n",
    "                                       borderMode=cv2.BORDER_CONSTANT, borderValue=255)\n",
    "            out.append(trans_img)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    for label in sorted(os.listdir(dir_path)):\n",
    "        lbl_path = os.path.join(dir_path, label)\n",
    "        if not os.path.isdir(lbl_path):\n",
    "            continue\n",
    "        for fname in os.listdir(lbl_path):\n",
    "            if not fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                continue\n",
    "            img = cv2.imread(os.path.join(lbl_path, fname), cv2.IMREAD_GRAYSCALE)\n",
    "            if img is None:\n",
    "                continue\n",
    "            # Resize to 28x28\n",
    "            img = cv2.resize(img, win_size)\n",
    "            imgs = augment_image(img) if augment else [img]\n",
    "            for im in imgs:\n",
    "                # Extract HOG features\n",
    "                desc = hog.compute(im)\n",
    "                X.append(desc.flatten())\n",
    "                y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "print(\"Loading and augmenting data...\")\n",
    "X, y = load_data(data_dir)\n",
    "print(f\"Total samples: {len(y)}, feature dim: {X.shape[1]}\")\n",
    "\n",
    "# 2. Encode labels and split\n",
    "y_encoder = LabelEncoder()\n",
    "y_enc = y_encoder.fit_transform(y)\n",
    "joblib.dump(y_encoder, os.path.join(output_dir, 'label_encoder.pkl'))\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_enc, test_size=0.2, stratify=y_enc, random_state=42\n",
    ")\n",
    "\n",
    "# 3. PCA for dimensionality reduction (without specifying n_components yet)\n",
    "pca = PCA(whiten=True, random_state=42)\n",
    "\n",
    "# 4. Define models and hyperparameters\n",
    "models = {\n",
    "    'DecisionTree': (DecisionTreeClassifier(random_state=42),\n",
    "                     {'pca__n_components': [0.95, 0.99],\n",
    "                      'clf__criterion': ['gini', 'entropy'],\n",
    "                      'clf__max_depth': [10, 20, None],\n",
    "                      'clf__min_samples_leaf': [1, 2, 5]}),\n",
    "    'SVM': (SVC(kernel='rbf', probability=True, random_state=42),\n",
    "            [{'pca__n_components': [0.95, 0.99],\n",
    "              'clf__kernel': ['rbf'],\n",
    "              'clf__C': [0.1, 1, 10],\n",
    "              'clf__gamma': ['scale', 'auto']},\n",
    "             {'pca__n_components': [0.95, 0.99],\n",
    "              'clf__kernel': ['linear'],\n",
    "              'clf__C': [0.1, 1, 10]}])\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, (estimator, params) in models.items():\n",
    "    pipe = Pipeline([('scaler', StandardScaler()), ('pca', pca), ('clf', estimator)])\n",
    "    grid = GridSearchCV(pipe, params, cv=5, n_jobs=-1, verbose=1)\n",
    "    print(f\"Training {name}...\")\n",
    "    grid.fit(X_train, y_train)\n",
    "    results[name] = grid\n",
    "    joblib.dump(grid.best_estimator_, os.path.join(output_dir, f\"{name.lower()}.pkl\"))\n",
    "    print(f\"{name} best params: {grid.best_params_}\")\n",
    "\n",
    "# 5. Evaluation and accuracy reporting\n",
    "for name, grid in results.items():\n",
    "    y_pred = grid.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{name} Accuracy: {acc:.4f}\")\n",
    "\n",
    "    # Detailed report\n",
    "    print(classification_report(y_test, y_pred, target_names=y_encoder.classes_))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    np.savetxt(os.path.join(output_dir, f\"cm_{name}.csv\"), cm, fmt='%d', delimiter=',')\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=y_encoder.classes_, yticklabels=y_encoder.classes_)\n",
    "    plt.title(f\"{name} Confusion Matrix\")\n",
    "    plt.xlabel('Predicted'); plt.ylabel('True')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f\"heatmap_{name}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "print(\"All done. Accuracies reported above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f94fb6",
   "metadata": {},
   "source": [
    "phase 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "66b7cb5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognized 100_plate: 36-three28-m32-ye36-three13-z10-d28-m28-m3-p\n",
      "Recognized 101_plate: 9-kh35-two6-jim40-seven9-kh6-jim\n",
      "Recognized 102_plate: 35-two28-m30-v35-two28-m28-m19-t-long28-m\n",
      "Recognized 103_plate: 22-ghyin28-m27-le6-jim28-m\n",
      "Recognized 104_plate: 41-eight41-eight17-sad41-eight28-m36-three35-two28-m\n",
      "Recognized 105_plate: 40-seven19-t-long28-m40-seven35-two35-two19-t-long40-seven\n",
      "Recognized 106_plate: 42-nine35-two8-h33-zero28-m43-anewfive19-t-long19-t-long28-m\n",
      "Recognized 107_plate: 35-two41-eight10-d41-eight43-anewfive35-two34-one40-seven\n",
      "Recognized 108_plate: 40-seven40-seven23-f41-eight40-seven3-p34-one3-p40-seven\n",
      "Recognized 109_plate: 35-two10-d29-n11-zal39-six35-two35-two34-one28-m\n",
      "Recognized 10_plate: 28-m9-kh10-d6-jim13-z35-two13-z39-six\n",
      "Recognized 110_plate: 28-m42-nine32-ye33-zero41-eight27-le42-nine34-one28-m\n",
      "Recognized 111_plate: 36-three41-eight27-le34-one35-two43-anewfive34-one40-seven\n",
      "Recognized 112_plate: 34-one19-t-long17-sad42-nine36-three22-ghyin28-m\n",
      "Recognized 113_plate: 40-seven35-two15-sin39-six34-one35-two19-t-long28-m40-seven\n",
      "Recognized 114_plate: 40-seven27-le28-m8-h35-two19-t-long40-seven\n",
      "Recognized 115_plate: 22-ghyin17-sad19-t-long41-eight43-anewfive\n",
      "Recognized 116_plate: 41-eight28-m19-t-long35-two38-five35-two19-t-long28-m\n",
      "Recognized 117_plate: 2-b19-t-long39-six19-t-long42-nine42-nine41-eight19-t-long40-seven\n",
      "Recognized 118_plate: 35-two42-nine5-se43-anewfive35-two43-anewfive19-t-long40-seven\n",
      "Recognized 119_plate: 42-nine37-four18-zad39-six28-m28-m35-two\n",
      "Recognized 11_plate: 27-le32-ye30-v28-m28-m14-zh35-two\n",
      "Recognized 120_plate: 30-v38-five27-le41-eight35-two41-eight30-v40-seven\n",
      "Recognized 121_plate: 35-two39-six17-sad35-two35-two41-eight35-two20-z-long\n",
      "Recognized 122_plate: 1-alef34-one34-one23-f41-eight41-eight39-six35-two28-m\n",
      "Recognized 123_plate: 42-nine43-anewfive32-ye43-anewfive42-nine35-two34-one\n",
      "Recognized 124_plate: 35-two35-two28-m35-two41-eight40-seven15-sin34-one6-jim28-m\n",
      "Recognized 125_plate: 34-one10-d35-two28-m43-anewfive34-one28-m\n",
      "Recognized 126_plate: 43-anewfive39-six10-d42-nine39-six27-le30-v40-seven\n",
      "Recognized 127_plate: 40-seven42-nine29-n11-zal40-seven43-anewfive40-seven3-p19-t-long18-zad40-seven\n",
      "Recognized 128_plate: 34-one35-two28-m42-nine36-three43-anewfive30-v28-m\n",
      "Recognized 129_plate: 42-nine41-eight17-sad41-eight39-six34-one19-t-long\n",
      "Recognized 12_plate: 3-p34-one19-t-long22-ghyin43-anewfive35-two28-m19-t-long14-zh\n",
      "Recognized 130_plate: 19-t-long33-zero7-che42-nine39-six42-nine28-m\n",
      "Recognized 131_plate: 39-six42-nine3-p39-six41-eight28-m19-t-long40-seven\n",
      "Recognized 132_plate: 28-m40-seven10-d40-seven42-nine41-eight19-t-long28-m\n",
      "Recognized 133_plate: 35-two18-zad39-six41-eight40-seven11-zal\n",
      "Recognized 134_plate: 42-nine34-one10-d39-six35-two35-two34-one28-m\n",
      "Recognized 135_plate: 35-two28-m3-p9-kh39-six28-m34-one28-m\n",
      "Recognized 136_plate: 40-seven28-m42-nine42-nine22-ghyin\n",
      "Recognized 137_plate: 39-six34-one28-m35-two43-anewfive14-zh28-m\n",
      "Recognized 138_plate: 35-two28-m28-m42-nine9-kh39-six4-t10-d\n",
      "Recognized 139_plate: 16-shin\n",
      "Recognized 13_plate: 28-m39-six32-ye33-zero28-m28-m42-nine34-one28-m\n",
      "Recognized 140_plate: 28-m40-seven40-seven\n",
      "Recognized 141_plate: 43-anewfive10-d35-two35-two28-m19-t-long28-m\n",
      "Recognized 142_plate: 41-eight41-eight8-h33-zero35-two41-eight42-nine19-t-long40-seven\n",
      "No character directory for 143_plate\n",
      "Recognized 144_plate: 11-zal35-two21-ayin33-zero35-two38-five42-nine16-shin35-two\n",
      "Recognized 145_plate: 36-three28-m31-he35-two41-eight28-m40-seven40-seven\n",
      "Recognized 146_plate: 43-anewfive40-seven29-n24-ghe34-one42-nine35-two14-zh40-seven\n",
      "Recognized 147_plate: 8-h33-zero35-two27-le42-nine19-t-long40-seven\n",
      "Recognized 148_plate: 35-two39-six10-d41-eight39-six41-eight40-seven\n",
      "Recognized 149_plate: 41-eight41-eight8-h9-kh37-four41-eight19-t-long28-m\n",
      "Recognized 14_plate: 28-m28-m3-p28-m35-two35-two19-t-long28-m\n",
      "Recognized 150_plate: 28-m30-v10-d42-nine\n",
      "Recognized 151_plate: 35-two34-one19-t-long19-t-long42-nine35-two14-zh28-m\n",
      "Recognized 152_plate: 34-one28-m27-le43-anewfive27-le19-t-long30-v\n",
      "Recognized 153_plate: 34-one39-six8-h33-zero14-zh28-m28-m1-alef28-m\n",
      "Recognized 154_plate: 35-two23-f23-f35-two28-m28-m4-t6-jim28-m\n",
      "Recognized 155_plate: 35-two35-two19-t-long28-m42-nine28-m\n",
      "Recognized 156_plate: 41-eight29-n38-five33-zero39-six40-seven40-seven40-seven\n",
      "Recognized 157_plate: 2-b36-three6-jim15-sin28-m35-two36-three\n",
      "Recognized 158_plate: 36-three39-six20-z-long42-nine35-two35-two19-t-long40-seven\n",
      "Recognized 159_plate: 16-shin\n",
      "Recognized 15_plate: 6-jim6-jim20-z-long6-jim42-nine30-v6-jim35-two\n",
      "Recognized 160_plate: 35-two28-m3-p33-zero6-jim39-six28-m16-shin34-one28-m\n",
      "Recognized 161_plate: 34-one43-anewfive28-m35-two35-two41-eight34-one27-le28-m\n",
      "Recognized 162_plate: 39-six18-zad40-seven26-g41-eight30-v\n",
      "Recognized 163_plate: 42-nine22-ghyin28-m37-four40-seven5-se6-jim10-d\n",
      "Recognized 164_plate: 42-nine37-four21-ayin33-zero37-four34-one28-m34-one\n",
      "Recognized 165_plate: 22-ghyin35-two6-jim35-two20-z-long16-shin\n",
      "Recognized 166_plate: 35-two35-two30-v27-le4-t\n",
      "Recognized 167_plate: 35-two43-anewfive24-ghe28-m41-eight28-m35-two40-seven\n",
      "Recognized 168_plate: 9-kh35-two25-k7-che41-eight35-two41-eight\n",
      "Recognized 169_plate: 43-anewfive35-two18-zad28-m41-eight35-two4-t19-t-long40-seven\n",
      "Recognized 16_plate: 3-p42-nine1-alef27-le43-anewfive43-anewfive36-three34-one28-m\n",
      "Recognized 170_plate: 34-one42-nine8-h33-zero40-seven34-one36-three\n",
      "Recognized 171_plate: 9-kh\n",
      "No character directory for 172_plate\n",
      "Recognized 173_plate: 34-one28-m19-t-long28-m9-kh36-three35-two34-one28-m37-four\n",
      "Recognized 174_plate: 42-nine34-one28-m\n",
      "Recognized 175_plate: 28-m27-le36-three36-three19-t-long14-zh28-m\n",
      "Recognized 176_plate: 6-jim24-ghe35-two6-jim37-four35-two\n",
      "Recognized 177_plate: 3-p28-m19-t-long28-m28-m28-m27-le28-m\n",
      "Recognized 178_plate: 28-m17-sad28-m6-jim35-two30-v40-seven\n",
      "Recognized 179_plate: 19-t-long3-p36-three36-three21-ayin26-g4-t\n",
      "Recognized 17_plate: 11-zal\n",
      "Recognized 180_plate: 40-seven10-d40-seven19-t-long42-nine28-m28-m\n",
      "Recognized 181_plate: 35-two24-ghe34-one41-eight43-anewfive34-one28-m\n",
      "Recognized 182_plate: 37-four41-eight30-v40-seven35-two39-six\n",
      "Recognized 183_plate: 35-two39-six29-n40-seven28-m35-two30-v28-m\n",
      "Recognized 184_plate: 19-t-long35-two\n",
      "Recognized 185_plate: 34-one41-eight8-h33-zero6-jim36-three42-nine30-v35-two\n",
      "Recognized 186_plate: 33-zero19-t-long9-kh10-d28-m\n",
      "Recognized 187_plate: 35-two6-jim27-le40-seven6-jim28-m1-alef34-one11-zal28-m\n",
      "Recognized 188_plate: 35-two41-eight10-d42-nine35-two\n",
      "Recognized 189_plate: 30-v35-two32-ye41-eight28-m43-anewfive35-two33-zero\n",
      "Recognized 18_plate: 6-jim41-eight40-seven19-t-long35-two15-sin27-le35-two19-t-long40-seven\n",
      "Recognized 190_plate: 42-nine35-two3-p35-two41-eight41-eight19-t-long\n",
      "Recognized 191_plate: 34-one35-two41-eight35-two43-anewfive32-ye\n",
      "Recognized 192_plate: 40-seven28-m17-sad35-two39-six42-nine19-t-long28-m\n",
      "Recognized 193_plate: 43-anewfive28-m19-t-long43-anewfive41-eight41-eight19-t-long28-m\n",
      "Recognized 194_plate: 36-three43-anewfive17-sad35-two35-two41-eight19-t-long40-seven\n",
      "Recognized 195_plate: 28-m17-sad42-nine39-six34-one22-ghyin\n",
      "Recognized 196_plate: 41-eight28-m17-sad41-eight42-nine42-nine19-t-long28-m\n",
      "Recognized 197_plate: 17-sad42-nine16-shin\n",
      "Recognized 198_plate: 41-eight43-anewfive15-sin34-one35-two42-nine34-one28-m\n",
      "Recognized 199_plate: 40-seven42-nine2-b40-seven39-six42-nine19-t-long40-seven\n",
      "Recognized 19_plate: 35-two41-eight29-n34-one42-nine34-one19-t-long40-seven\n",
      "Recognized 1_plate: 34-one9-kh15-sin34-one42-nine10-d28-m\n",
      "Recognized 200_plate: 41-eight43-anewfive17-sad39-six28-m28-m\n",
      "Recognized 201_plate: 33-zero30-v28-m\n",
      "Recognized 202_plate: 35-two27-le28-m35-two34-one28-m\n",
      "Recognized 203_plate: 36-three39-six19-t-long35-two14-zh37-four\n",
      "Recognized 204_plate: 40-seven22-ghyin14-zh\n",
      "Recognized 205_plate: 39-six35-two32-ye33-zero34-one40-seven39-six19-t-long28-m\n",
      "Recognized 206_plate: 34-one28-m41-eight35-two34-one34-one28-m\n",
      "Recognized 207_plate: 39-six39-six27-le35-two42-nine40-seven30-v40-seven\n",
      "Recognized 208_plate: 40-seven35-two27-le6-jim28-m28-m\n",
      "Recognized 209_plate: 35-two17-sad40-seven40-seven35-two4-t19-t-long15-sin40-seven\n",
      "Recognized 20_plate: 41-eight40-seven27-le40-seven19-t-long40-seven15-sin28-m40-seven\n",
      "Recognized 210_plate: 42-nine28-m8-h33-zero28-m28-m35-two19-t-long28-m\n",
      "Recognized 211_plate: 28-m35-two27-le35-two35-two41-eight26-g34-one10-d40-seven\n",
      "Recognized 212_plate: 38-five6-jim8-h33-zero40-seven42-nine38-five28-m40-seven\n",
      "Recognized 213_plate: 6-jim35-two10-d14-zh39-six6-jim\n",
      "Recognized 214_plate: 35-two28-m27-le28-m28-m27-le\n",
      "Recognized 215_plate: 41-eight35-two3-p39-six35-two34-one35-two28-m\n",
      "Recognized 216_plate: 35-two41-eight27-le35-two35-two19-t-long28-m\n",
      "Recognized 217_plate: 41-eight35-two3-p39-six37-four34-one40-seven\n",
      "Recognized 21_plate: 38-five35-two32-ye35-two34-one34-one34-one40-seven\n",
      "Recognized 22_plate: 39-six35-two32-ye33-zero41-eight37-four38-five34-one28-m\n",
      "Recognized 23_plate: 19-t-long39-six6-jim39-six35-two14-zh28-m\n",
      "Recognized 24_plate: 37-four28-m8-h33-zero28-m42-nine37-four19-t-long40-seven\n",
      "Recognized 25_plate: 26-g6-jim10-d39-six30-v28-m28-m\n",
      "Recognized 26_plate: 9-kh6-jim11-zal16-shin6-jim33-zero34-one18-zad41-eight17-sad19-t-long\n",
      "Recognized 27_plate: 35-two42-nine8-h33-zero43-anewfive37-four9-kh3-p4-t\n",
      "Recognized 28_plate: 43-anewfive34-one28-m42-nine42-nine28-m16-shin34-one28-m\n",
      "Recognized 29_plate: 43-anewfive43-anewfive35-two41-eight41-eight19-t-long4-t6-jim15-sin7-che\n",
      "Recognized 2_plate: 40-seven10-d35-two41-eight40-seven40-seven\n",
      "Recognized 30_plate: 35-two35-two30-v35-two35-two35-two6-jim28-m\n",
      "Recognized 31_plate: 43-anewfive35-two19-t-long41-eight35-two28-m40-seven\n",
      "Recognized 32_plate: 41-eight28-m28-m35-two35-two27-le34-one40-seven\n",
      "Recognized 33_plate: 35-two19-t-long17-sad42-nine19-t-long43-anewfive6-jim35-two\n",
      "Recognized 34_plate: 34-one28-m1-alef39-six43-anewfive6-jim34-one40-seven\n",
      "Recognized 35_plate: 35-two28-m17-sad42-nine43-anewfive43-anewfive19-t-long40-seven\n",
      "Recognized 36_plate: 16-shin36-three\n",
      "Recognized 37_plate: 35-two39-six10-d34-one34-one39-six34-one40-seven\n",
      "Recognized 38_plate: 34-one43-anewfive29-n35-two41-eight40-seven34-one35-two\n",
      "Recognized 39_plate: 18-zad19-t-long28-m\n",
      "Recognized 3_plate: 35-two35-two3-p35-two35-two9-kh43-anewfive43-anewfive\n",
      "Recognized 40_plate: 40-seven9-kh8-h33-zero35-two19-t-long40-seven19-t-long40-seven\n",
      "Recognized 41_plate: 34-one36-three10-d36-three39-six36-three34-one28-m\n",
      "Recognized 42_plate: 35-two36-three24-ghe34-one9-kh43-anewfive34-one28-m\n",
      "Recognized 43_plate: 35-two39-six30-v39-six28-m34-one19-t-long28-m\n",
      "Recognized 44_plate: 41-eight35-two17-sad35-two39-six41-eight30-v35-two\n",
      "Recognized 45_plate: 34-one39-six30-v42-nine36-three40-seven19-t-long40-seven\n",
      "No character directory for 46_plate\n",
      "Recognized 47_plate: 28-m35-two9-kh33-zero28-m6-jim34-one28-m\n",
      "Recognized 48_plate: 34-one17-sad1-alef39-six41-eight19-t-long28-m\n",
      "Recognized 49_plate: 35-two28-m29-n35-two41-eight28-m40-seven40-seven\n",
      "Recognized 4_plate: 35-two43-anewfive27-le35-two42-nine40-seven34-one40-seven\n",
      "Recognized 50_plate: 24-ghe37-four32-ye13-z41-eight3-p18-zad6-jim18-zad38-five\n",
      "Recognized 51_plate: 43-anewfive35-two32-ye33-zero35-two28-m42-nine\n",
      "Recognized 52_plate: 43-anewfive40-seven12-r43-anewfive40-seven41-eight6-jim28-m\n",
      "Recognized 53_plate: 18-zad19-t-long40-seven\n",
      "Recognized 54_plate: 35-two39-six29-n34-one28-m40-seven34-one40-seven\n",
      "Recognized 55_plate: 35-two28-m17-sad27-le39-six28-m30-v\n",
      "Recognized 56_plate: 6-jim8-h33-zero6-jim41-eight6-jim\n",
      "Recognized 57_plate: 40-seven19-t-long29-n34-one10-d43-anewfive34-one40-seven\n",
      "Recognized 58_plate: 19-t-long33-zero30-v39-six41-eight19-t-long19-t-long40-seven\n",
      "Recognized 59_plate: 17-sad40-seven35-two\n",
      "Recognized 5_plate: 40-seven28-m41-eight27-le35-two19-t-long40-seven\n",
      "Recognized 60_plate: 39-six35-two17-sad42-nine40-seven42-nine\n",
      "Recognized 61_plate: 43-anewfive41-eight19-t-long34-one28-m39-six34-one\n",
      "Recognized 62_plate: 43-anewfive35-two27-le43-anewfive36-three35-two34-one28-m\n",
      "Recognized 63_plate: 19-t-long41-eight28-m39-six40-seven39-six19-t-long40-seven\n",
      "Recognized 64_plate: 6-jim27-le35-two28-m28-m28-m\n",
      "Recognized 65_plate: 20-z-long20-z-long29-n39-six27-le\n",
      "Recognized 66_plate: 28-m41-eight32-ye33-zero28-m39-six35-two10-d28-m\n",
      "Recognized 67_plate: 35-two10-d34-one35-two28-m19-t-long28-m\n",
      "Recognized 68_plate: 35-two39-six18-zad43-anewfive39-six36-three19-t-long40-seven\n",
      "Recognized 69_plate: 41-eight35-two1-alef35-two42-nine42-nine34-one34-one\n",
      "Recognized 6_plate: 39-six41-eight28-m35-two42-nine35-two19-t-long40-seven\n",
      "Recognized 70_plate: 35-two42-nine16-shin28-m42-nine41-eight34-one28-m\n",
      "Recognized 71_plate: 41-eight40-seven6-jim35-two37-four42-nine28-m40-seven\n",
      "Recognized 72_plate: 2-b42-nine19-t-long18-zad41-eight28-m38-five19-t-long40-seven\n",
      "Recognized 73_plate: 43-anewfive34-one15-sin28-m43-anewfive9-kh28-m28-m\n",
      "Recognized 74_plate: 39-six35-two13-z35-two35-two40-seven34-one40-seven\n",
      "Recognized 75_plate: 6-jim35-two16-shin38-five42-nine6-jim6-jim28-m\n",
      "Recognized 76_plate: 43-anewfive41-eight17-sad38-five39-six41-eight34-one28-m\n",
      "Recognized 77_plate: 35-two35-two27-le35-two28-m41-eight34-one35-two\n",
      "Recognized 78_plate: 40-seven5-se42-nine41-eight41-eight40-seven\n",
      "Recognized 79_plate: 39-six42-nine24-ghe24-ghe39-six28-m28-m4-t34-one40-seven\n",
      "Recognized 7_plate: 36-three39-six27-le42-nine27-le40-seven34-one40-seven\n",
      "Recognized 80_plate: 35-two28-m30-v35-two9-kh43-anewfive4-t6-jim24-ghe26-g35-two\n",
      "Recognized 81_plate: 40-seven19-t-long20-z-long41-eight38-five40-seven34-one40-seven\n",
      "Recognized 82_plate: 3-p37-four28-m30-v35-two34-one43-anewfive19-t-long28-m\n",
      "Recognized 83_plate: 3-p35-two35-two17-sad35-two28-m34-one30-v3-p15-sin35-two\n",
      "Recognized 84_plate: 35-two6-jim8-h33-zero35-two35-two30-v4-t17-sad28-m\n",
      "Recognized 85_plate: 42-nine34-one10-d6-jim28-m28-m30-v\n",
      "Recognized 86_plate: 19-t-long42-nine32-ye23-f35-two40-seven28-m34-one35-two\n",
      "Recognized 87_plate: 35-two34-one10-d28-m\n",
      "Recognized 88_plate: 33-zero39-six10-d42-nine43-anewfive36-three19-t-long28-m\n",
      "Recognized 89_plate: 40-seven39-six10-d19-t-long19-t-long39-six34-one40-seven\n",
      "Recognized 8_plate: 29-n40-seven42-nine19-t-long40-seven\n",
      "Recognized 90_plate: 34-one35-two30-v43-anewfive35-two39-six34-one\n",
      "Recognized 91_plate: 41-eight41-eight29-n35-two35-two35-two34-one40-seven\n",
      "Recognized 92_plate: 35-two34-one28-m42-nine40-seven35-two30-v26-g40-seven\n",
      "Recognized 93_plate: 3-p9-kh22-ghyin9-kh42-nine6-jim6-jim42-nine\n",
      "Recognized 94_plate: 35-two41-eight16-shin28-m42-nine36-three34-one28-m\n",
      "Recognized 95_plate: 40-seven39-six30-v39-six40-seven19-t-long19-t-long40-seven\n",
      "Recognized 96_plate: 42-nine39-six17-sad28-m34-one39-six35-two28-m\n",
      "Recognized 97_plate: 34-one6-jim10-d42-nine34-one36-three34-one28-m\n",
      "Recognized 98_plate: 27-le\n",
      "Recognized 99_plate: 35-two37-four27-le41-eight19-t-long42-nine22-ghyin40-seven\n",
      "Recognized 9_plate: 34-one35-two27-le35-two35-two19-t-long19-t-long40-seven\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Paths\n",
    "dirs = {\n",
    "    'plate': r'plates33',\n",
    "    'chars': r'characters9',\n",
    "    'models': r'models_flat30',\n",
    "    'output': r'plate_results'\n",
    "}\n",
    "for d in dirs.values(): os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# Load models and encoder\n",
    "dt = joblib.load(os.path.join(dirs['models'], 'decisiontree.pkl'))\n",
    "svm = joblib.load(os.path.join(dirs['models'], 'svm.pkl'))\n",
    "rf_path = os.path.join(dirs['models'], 'randomforest.pkl')\n",
    "rf = joblib.load(rf_path) if os.path.isfile(rf_path) else None\n",
    "le = joblib.load(os.path.join(dirs['models'], 'label_encoder.pkl'))\n",
    "\n",
    "# HOG descriptor - MATCHED TO TRAINING PARAMETERS\n",
    "hog = cv2.HOGDescriptor(\n",
    "    _winSize=(28, 28),         # Match training size\n",
    "    _blockSize=(14, 14),       # Match training\n",
    "    _blockStride=(7, 7),       # Match training\n",
    "    _cellSize=(7, 7),          # Match training\n",
    "    _nbins=9\n",
    ")\n",
    "\n",
    "def compute_hog(img):\n",
    "    if img.ndim == 3:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Resize to 28x28 to match training\n",
    "    img = cv2.resize(img, (28, 28))\n",
    "    \n",
    "    # Apply same preprocessing as training (no explicit thresholding in training)\n",
    "    # If you still want thresholding, uncomment next line\n",
    "    # _, img = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    return hog.compute(img).flatten().reshape(1,-1)\n",
    "\n",
    "# Iterate plates\n",
    "for pf in os.listdir(dirs['plate']):\n",
    "    name, _ = os.path.splitext(pf)\n",
    "    char_dir = os.path.join(dirs['chars'], name)\n",
    "    if not os.path.isdir(char_dir): \n",
    "        print(f\"No character directory for {name}\")\n",
    "        continue\n",
    "    \n",
    "    # sorted char files\n",
    "    files = sorted([f for f in os.listdir(char_dir) if f.startswith('char')],\n",
    "                   key=lambda x: int(''.join(filter(str.isdigit,x)) or 0))\n",
    "    \n",
    "    if not files:\n",
    "        print(f\"No character files found in {char_dir}\")\n",
    "        continue\n",
    "        \n",
    "    preds = {'dt':[], 'svm':[], 'rf':[], 'vote':[]}\n",
    "    \n",
    "    for f in files:\n",
    "        path = os.path.join(char_dir, f)\n",
    "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None:\n",
    "            print(f\"Failed to read {path}\")\n",
    "            for k in preds: preds[k].append('_')\n",
    "            continue\n",
    "            \n",
    "        # Compute HOG features with matched parameters\n",
    "        feat = compute_hog(img)\n",
    "        \n",
    "        try:\n",
    "            # DT prediction\n",
    "            dt_idx = dt.predict(feat)[0]\n",
    "            dt_c = le.inverse_transform([dt_idx])[0]\n",
    "            preds['dt'].append(dt_c)\n",
    "            \n",
    "            # SVM prediction\n",
    "            svm_idx = svm.predict(feat)[0]\n",
    "            svm_c = le.inverse_transform([svm_idx])[0]\n",
    "            preds['svm'].append(svm_c)\n",
    "            \n",
    "            # RF prediction if available\n",
    "            if rf:\n",
    "                rf_idx = rf.predict(feat)[0]\n",
    "                rf_c = le.inverse_transform([rf_idx])[0]\n",
    "                preds['rf'].append(rf_c)\n",
    "            else:\n",
    "                preds['rf'].append('_')\n",
    "                \n",
    "            # Ensemble voting with probabilities\n",
    "            probs = np.zeros((feat.shape[0], len(le.classes_)))\n",
    "            probs += dt.predict_proba(feat)\n",
    "            probs += svm.predict_proba(feat)\n",
    "            if rf: probs += rf.predict_proba(feat)\n",
    "            \n",
    "            avg = probs / (3 if rf else 2)\n",
    "            vidx = np.argmax(avg, axis=1)[0]\n",
    "            vprob = avg[0, vidx]\n",
    "            vote_c = le.inverse_transform([vidx])[0] if vprob >= 0.5 else '?'  # Lowered threshold slightly\n",
    "            preds['vote'].append(vote_c)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {path}: {str(e)}\")\n",
    "            for k in preds: preds[k].append('?')\n",
    "    \n",
    "    # Create final output\n",
    "    final = ''.join(preds['vote'])\n",
    "    \n",
    "    # Save results\n",
    "    out = os.path.join(dirs['output'], f\"{name}_result.txt\")\n",
    "    with open(out, 'w', encoding='utf-8') as f:\n",
    "        f.write('DT:' + ','.join(preds['dt']) + '\\n')\n",
    "        f.write('SVM:' + ','.join(preds['svm']) + '\\n')\n",
    "        if rf: f.write('RF:' + ','.join(preds['rf']) + '\\n')\n",
    "        f.write('VOTE:' + ','.join(preds['vote']) + '\\n')\n",
    "        f.write('FINAL:' + final)\n",
    "        \n",
    "    print(f\"Recognized {name}: {final}\")\n",
    "    \n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c6260a",
   "metadata": {},
   "source": [
    "phase5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "47a22b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plate 1: exact_match=False, sequence_pct=12.5%\n",
      "Plate 10: exact_match=False, sequence_pct=14.3%\n",
      "Plate 100: exact_match=False, sequence_pct=25.0%\n",
      "Plate 101: exact_match=False, sequence_pct=0.0%\n",
      "Plate 102: exact_match=False, sequence_pct=25.0%\n",
      "Plate 103: exact_match=False, sequence_pct=12.5%\n",
      "Plate 104: exact_match=False, sequence_pct=62.5%\n",
      "Plate 105: exact_match=False, sequence_pct=62.5%\n",
      "Plate 106: exact_match=False, sequence_pct=12.5%\n",
      "Plate 107: exact_match=False, sequence_pct=62.5%\n",
      "Plate 108: exact_match=False, sequence_pct=25.0%\n",
      "Plate 109: exact_match=False, sequence_pct=12.5%\n",
      "Plate 11: exact_match=False, sequence_pct=0.0%\n",
      "Plate 110: exact_match=False, sequence_pct=12.5%\n",
      "Plate 111: exact_match=False, sequence_pct=87.5%\n",
      "Plate 112: exact_match=False, sequence_pct=25.0%\n",
      "Plate 113: exact_match=False, sequence_pct=62.5%\n",
      "Plate 114: exact_match=False, sequence_pct=0.0%\n",
      "Plate 115: exact_match=False, sequence_pct=0.0%\n",
      "Plate 116: exact_match=False, sequence_pct=50.0%\n",
      "Plate 117: exact_match=False, sequence_pct=12.5%\n",
      "Plate 118: exact_match=False, sequence_pct=37.5%\n",
      "Plate 119: exact_match=False, sequence_pct=50.0%\n",
      "Plate 12: exact_match=False, sequence_pct=0.0%\n",
      "Plate 120: exact_match=False, sequence_pct=62.5%\n",
      "Plate 121: exact_match=False, sequence_pct=75.0%\n",
      "Plate 122: exact_match=False, sequence_pct=25.0%\n",
      "Plate 123: exact_match=False, sequence_pct=37.5%\n",
      "Plate 124: exact_match=False, sequence_pct=50.0%\n",
      "Plate 125: exact_match=False, sequence_pct=0.0%\n",
      "Plate 126: exact_match=False, sequence_pct=62.5%\n",
      "Plate 127: exact_match=False, sequence_pct=25.0%\n",
      "Plate 128: exact_match=False, sequence_pct=37.5%\n",
      "Plate 129: exact_match=False, sequence_pct=75.0%\n",
      "Plate 13: exact_match=False, sequence_pct=12.5%\n",
      "Plate 130: exact_match=False, sequence_pct=37.5%\n",
      "Plate 131: exact_match=False, sequence_pct=62.5%\n",
      "Plate 132: exact_match=False, sequence_pct=37.5%\n",
      "Plate 133: exact_match=False, sequence_pct=0.0%\n",
      "Plate 134: exact_match=False, sequence_pct=75.0%\n",
      "Plate 135: exact_match=False, sequence_pct=37.5%\n",
      "Plate 136: exact_match=False, sequence_pct=12.5%\n",
      "Plate 137: exact_match=False, sequence_pct=37.5%\n",
      "Plate 138: exact_match=False, sequence_pct=37.5%\n",
      "Plate 139: exact_match=False, sequence_pct=0.0%\n",
      "Plate 14: exact_match=False, sequence_pct=12.5%\n",
      "Plate 140: exact_match=False, sequence_pct=0.0%\n",
      "Plate 141: exact_match=False, sequence_pct=12.5%\n",
      "Plate 142: exact_match=False, sequence_pct=37.5%\n",
      "Missing result for 143\n",
      "Plate 144: exact_match=False, sequence_pct=12.5%\n",
      "Plate 145: exact_match=False, sequence_pct=75.0%\n",
      "Plate 146: exact_match=False, sequence_pct=0.0%\n",
      "Plate 147: exact_match=False, sequence_pct=0.0%\n",
      "Plate 149: exact_match=False, sequence_pct=25.0%\n",
      "Plate 15: exact_match=False, sequence_pct=12.5%\n",
      "Plate 150: exact_match=False, sequence_pct=25.0%\n",
      "Plate 151: exact_match=False, sequence_pct=25.0%\n",
      "Plate 152: exact_match=False, sequence_pct=25.0%\n",
      "Plate 153: exact_match=False, sequence_pct=12.5%\n",
      "Plate 154: exact_match=False, sequence_pct=12.5%\n",
      "Plate 155: exact_match=False, sequence_pct=0.0%\n",
      "Plate 156: exact_match=False, sequence_pct=37.5%\n",
      "Plate 157: exact_match=False, sequence_pct=12.5%\n",
      "Plate 158: exact_match=False, sequence_pct=62.5%\n",
      "Plate 159: exact_match=False, sequence_pct=0.0%\n",
      "Plate 16: exact_match=False, sequence_pct=12.5%\n",
      "Plate 160: exact_match=False, sequence_pct=12.5%\n",
      "Plate 161: exact_match=False, sequence_pct=62.5%\n",
      "Plate 162: exact_match=False, sequence_pct=0.0%\n",
      "Plate 163: exact_match=False, sequence_pct=12.5%\n",
      "Plate 164: exact_match=False, sequence_pct=12.5%\n",
      "Plate 165: exact_match=False, sequence_pct=0.0%\n",
      "Plate 166: exact_match=False, sequence_pct=25.0%\n",
      "Plate 167: exact_match=False, sequence_pct=37.5%\n",
      "Plate 168: exact_match=False, sequence_pct=0.0%\n",
      "Plate 169: exact_match=False, sequence_pct=12.5%\n",
      "Plate 17: exact_match=False, sequence_pct=0.0%\n",
      "Plate 170: exact_match=False, sequence_pct=37.5%\n",
      "Plate 171: exact_match=False, sequence_pct=0.0%\n",
      "Missing result for 172\n",
      "Plate 173: exact_match=False, sequence_pct=25.0%\n",
      "Plate 174: exact_match=False, sequence_pct=12.5%\n",
      "Plate 175: exact_match=False, sequence_pct=0.0%\n",
      "Plate 176: exact_match=False, sequence_pct=0.0%\n",
      "Plate 177: exact_match=False, sequence_pct=0.0%\n",
      "Plate 178: exact_match=False, sequence_pct=0.0%\n",
      "Plate 179: exact_match=False, sequence_pct=0.0%\n",
      "Plate 18: exact_match=False, sequence_pct=0.0%\n",
      "Plate 180: exact_match=False, sequence_pct=0.0%\n",
      "Plate 181: exact_match=False, sequence_pct=0.0%\n",
      "Plate 182: exact_match=False, sequence_pct=50.0%\n",
      "Plate 183: exact_match=False, sequence_pct=25.0%\n",
      "Plate 184: exact_match=False, sequence_pct=0.0%\n",
      "Plate 185: exact_match=False, sequence_pct=37.5%\n",
      "Plate 186: exact_match=False, sequence_pct=0.0%\n",
      "Plate 187: exact_match=False, sequence_pct=12.5%\n",
      "Plate 188: exact_match=False, sequence_pct=37.5%\n",
      "Plate 189: exact_match=False, sequence_pct=62.5%\n",
      "Plate 19: exact_match=False, sequence_pct=62.5%\n",
      "Plate 190: exact_match=False, sequence_pct=50.0%\n",
      "Plate 191: exact_match=False, sequence_pct=25.0%\n",
      "Plate 192: exact_match=False, sequence_pct=37.5%\n",
      "Plate 193: exact_match=False, sequence_pct=25.0%\n",
      "Plate 194: exact_match=False, sequence_pct=37.5%\n",
      "Plate 195: exact_match=False, sequence_pct=0.0%\n",
      "Plate 196: exact_match=False, sequence_pct=62.5%\n",
      "Plate 197: exact_match=False, sequence_pct=0.0%\n",
      "Plate 198: exact_match=False, sequence_pct=62.5%\n",
      "Plate 199: exact_match=False, sequence_pct=75.0%\n",
      "Plate 2: exact_match=False, sequence_pct=12.5%\n",
      "Plate 20: exact_match=False, sequence_pct=25.0%\n",
      "Plate 200: exact_match=False, sequence_pct=37.5%\n",
      "Plate 201: exact_match=False, sequence_pct=0.0%\n",
      "Plate 202: exact_match=False, sequence_pct=0.0%\n",
      "Plate 203: exact_match=False, sequence_pct=12.5%\n",
      "Plate 204: exact_match=False, sequence_pct=0.0%\n",
      "Plate 205: exact_match=False, sequence_pct=25.0%\n",
      "Plate 206: exact_match=False, sequence_pct=25.0%\n",
      "Plate 207: exact_match=False, sequence_pct=87.5%\n",
      "Plate 208: exact_match=False, sequence_pct=37.5%\n",
      "Plate 209: exact_match=False, sequence_pct=0.0%\n",
      "Plate 21: exact_match=False, sequence_pct=62.5%\n",
      "Plate 210: exact_match=False, sequence_pct=12.5%\n",
      "Plate 211: exact_match=False, sequence_pct=25.0%\n",
      "Plate 212: exact_match=False, sequence_pct=25.0%\n",
      "Plate 213: exact_match=False, sequence_pct=37.5%\n",
      "Plate 214: exact_match=False, sequence_pct=12.5%\n",
      "Plate 215: exact_match=False, sequence_pct=50.0%\n",
      "Plate 216: exact_match=False, sequence_pct=37.5%\n",
      "Plate 217: exact_match=False, sequence_pct=37.5%\n",
      "Plate 22: exact_match=False, sequence_pct=12.5%\n",
      "Plate 23: exact_match=False, sequence_pct=25.0%\n",
      "Plate 24: exact_match=False, sequence_pct=0.0%\n",
      "Plate 25: exact_match=False, sequence_pct=25.0%\n",
      "Plate 26: exact_match=False, sequence_pct=12.5%\n",
      "Plate 27: exact_match=False, sequence_pct=25.0%\n",
      "Plate 28: exact_match=False, sequence_pct=37.5%\n",
      "Plate 29: exact_match=False, sequence_pct=12.5%\n",
      "Plate 3: exact_match=False, sequence_pct=37.5%\n",
      "Plate 30: exact_match=False, sequence_pct=25.0%\n",
      "Plate 31: exact_match=False, sequence_pct=12.5%\n",
      "Plate 32: exact_match=False, sequence_pct=50.0%\n",
      "Plate 33: exact_match=False, sequence_pct=37.5%\n",
      "Plate 34: exact_match=False, sequence_pct=50.0%\n",
      "Plate 35: exact_match=False, sequence_pct=25.0%\n",
      "Plate 36: exact_match=False, sequence_pct=0.0%\n",
      "Plate 37: exact_match=False, sequence_pct=87.5%\n",
      "Plate 38: exact_match=False, sequence_pct=62.5%\n",
      "Plate 39: exact_match=False, sequence_pct=0.0%\n",
      "Plate 4: exact_match=False, sequence_pct=62.5%\n",
      "Plate 40: exact_match=False, sequence_pct=0.0%\n",
      "Plate 41: exact_match=False, sequence_pct=75.0%\n",
      "Plate 42: exact_match=False, sequence_pct=37.5%\n",
      "Plate 43: exact_match=False, sequence_pct=62.5%\n",
      "Plate 44: exact_match=False, sequence_pct=37.5%\n",
      "Plate 99: exact_match=False, sequence_pct=50.0%\n",
      "Plate 98: exact_match=False, sequence_pct=0.0%\n",
      "Plate 97: exact_match=False, sequence_pct=50.0%\n",
      "Plate 96: exact_match=False, sequence_pct=75.0%\n",
      "Plate 95: exact_match=False, sequence_pct=62.5%\n",
      "Plate 94: exact_match=False, sequence_pct=62.5%\n",
      "Plate 93: exact_match=False, sequence_pct=12.5%\n",
      "Plate 92: exact_match=False, sequence_pct=50.0%\n",
      "Plate 91: exact_match=False, sequence_pct=62.5%\n",
      "Plate 90: exact_match=False, sequence_pct=50.0%\n",
      "Plate 9: exact_match=False, sequence_pct=62.5%\n",
      "Plate 89: exact_match=False, sequence_pct=62.5%\n",
      "Plate 88: exact_match=False, sequence_pct=50.0%\n",
      "Plate 87: exact_match=False, sequence_pct=0.0%\n",
      "Plate 86: exact_match=False, sequence_pct=25.0%\n",
      "Plate 85: exact_match=False, sequence_pct=37.5%\n",
      "Plate 84: exact_match=False, sequence_pct=0.0%\n",
      "Plate 83: exact_match=False, sequence_pct=12.5%\n",
      "Plate 82: exact_match=False, sequence_pct=0.0%\n",
      "Plate 81: exact_match=False, sequence_pct=75.0%\n",
      "Plate 80: exact_match=False, sequence_pct=25.0%\n",
      "Plate 8: exact_match=False, sequence_pct=0.0%\n",
      "Plate 79: exact_match=False, sequence_pct=37.5%\n",
      "Plate 78: exact_match=False, sequence_pct=12.5%\n",
      "Plate 77: exact_match=False, sequence_pct=50.0%\n",
      "Plate 76: exact_match=False, sequence_pct=75.0%\n",
      "Plate 75: exact_match=False, sequence_pct=37.5%\n",
      "Plate 74: exact_match=False, sequence_pct=37.5%\n",
      "Plate 73: exact_match=False, sequence_pct=25.0%\n",
      "Plate 72: exact_match=False, sequence_pct=0.0%\n",
      "Plate 71: exact_match=False, sequence_pct=75.0%\n",
      "Plate 70: exact_match=False, sequence_pct=50.0%\n",
      "Plate 7: exact_match=False, sequence_pct=62.5%\n",
      "Plate 69: exact_match=False, sequence_pct=50.0%\n",
      "Plate 68: exact_match=False, sequence_pct=50.0%\n",
      "Plate 67: exact_match=False, sequence_pct=0.0%\n",
      "Plate 66: exact_match=False, sequence_pct=12.5%\n",
      "Plate 65: exact_match=False, sequence_pct=12.5%\n",
      "Plate 64: exact_match=False, sequence_pct=0.0%\n",
      "Plate 63: exact_match=False, sequence_pct=75.0%\n",
      "Plate 62: exact_match=False, sequence_pct=37.5%\n",
      "Plate 61: exact_match=False, sequence_pct=37.5%\n",
      "Plate 60: exact_match=True, sequence_pct=75.0%\n",
      "Plate 6: exact_match=False, sequence_pct=62.5%\n",
      "Plate 59: exact_match=False, sequence_pct=0.0%\n",
      "Plate 58: exact_match=False, sequence_pct=50.0%\n",
      "Plate 57: exact_match=False, sequence_pct=37.5%\n",
      "Plate 56: exact_match=False, sequence_pct=12.5%\n",
      "Plate 55: exact_match=False, sequence_pct=37.5%\n",
      "Plate 54: exact_match=False, sequence_pct=62.5%\n",
      "Plate 53: exact_match=False, sequence_pct=0.0%\n",
      "Plate 52: exact_match=False, sequence_pct=12.5%\n",
      "Plate 51: exact_match=False, sequence_pct=0.0%\n",
      "Plate 50: exact_match=False, sequence_pct=25.0%\n",
      "Plate 5: exact_match=False, sequence_pct=0.0%\n",
      "Plate 49: exact_match=False, sequence_pct=50.0%\n",
      "Plate 48: exact_match=False, sequence_pct=37.5%\n",
      "Plate 47: exact_match=False, sequence_pct=12.5%\n",
      "Missing result for 46\n",
      "Plate 45: exact_match=False, sequence_pct=62.5%\n",
      "\n",
      "Phase 5 evaluation complete.\n",
      "Plate exact-match (FINAL): 0.00%\n",
      "Plate exact-match (DT):    0.00%\n",
      "Plate exact-match (SVM):   0.00%\n",
      "Avg sequence acc (FINAL):  29.41%\n",
      "Token-level acc (FINAL):   33.22%\n",
      "Token-level acc (DT):      33.22%\n",
      "Token-level acc (SVM):     25.33%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Paths\n",
    "GROUND_TRUTH_CSV = r\"plate_labels.txt\"     # 'name,plate' where plate is e.g. \"6-jim,9-kh,…\"\n",
    "PREDICTION_DIR   = r\"plate_results\"        # contains <base>_plate_result.txt\n",
    "OUTPUT_DIR       = r\"evaluation_results\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def strip_prefix(token):\n",
    "    return token.split('-', 1)[-1]\n",
    "\n",
    "# 1. Load ground truth (strip numeric prefixes)\n",
    "gt_map = {}\n",
    "with open(GROUND_TRUTH_CSV, 'r', encoding='utf-8') as f:\n",
    "    lines = [ln.strip() for ln in f if ln.strip()]\n",
    "for ln in lines[1:]:\n",
    "    parts = ln.split(',')\n",
    "    if len(parts) < 2:\n",
    "        continue\n",
    "    name = parts[0].strip()\n",
    "    tail = ','.join(parts[1:]).strip()\n",
    "    raw_tokens = [t.strip() for t in tail.split(',') if t.strip()]\n",
    "    gt_map[name] = [strip_prefix(t) for t in raw_tokens]\n",
    "\n",
    "# 2. Iterate plates and compute metrics\n",
    "plate_total = 0\n",
    "plate_exact_matches = 0\n",
    "plate_exact_matches_dt = 0\n",
    "plate_exact_matches_svm = 0\n",
    "sequence_scores = []       # per-plate % correct in sequence for FINAL\n",
    "all_gt, all_pred = [], []  # For FINAL predictions\n",
    "all_pred_dt = []           # For DT predictions\n",
    "all_pred_svm = []          # For SVM predictions\n",
    "\n",
    "for name, gt_labels in gt_map.items():\n",
    "    base = name.split('_')[0]\n",
    "    res_file = os.path.join(PREDICTION_DIR, f\"{base}_plate_result.txt\")\n",
    "    if not os.path.isfile(res_file):\n",
    "        print(f\"Missing result for {name}\")\n",
    "        continue\n",
    "\n",
    "    with open(res_file, 'r', encoding='utf-8') as f:\n",
    "        lines = [ln.strip() for ln in f if ln.strip()]\n",
    "\n",
    "    # Extract lines for FINAL, DT, and SVM\n",
    "    final_line = next((ln for ln in lines if ln.startswith('FINAL:')), None)\n",
    "    dt_line = next((ln for ln in lines if ln.startswith('DT:')), None)\n",
    "    svm_line = next((ln for ln in lines if ln.startswith('SVM:')), None)\n",
    "\n",
    "    if not final_line or not dt_line or not svm_line:\n",
    "        print(f\"Missing FINAL, DT, or SVM line in {res_file}\")\n",
    "        continue\n",
    "\n",
    "    # Process FINAL predictions (original)\n",
    "    raw_pred = re.findall(r'\\d+-[^\\d,]+', final_line)\n",
    "    pred_labels = [strip_prefix(t) for t in raw_pred]\n",
    "\n",
    "    # Process DT predictions\n",
    "    raw_pred_dt = re.findall(r'\\d+-[^\\d,]+', dt_line)\n",
    "    pred_labels_dt = [strip_prefix(t) for t in raw_pred_dt]\n",
    "\n",
    "    # Process SVM predictions\n",
    "    raw_pred_svm = re.findall(r'\\d+-[^\\d,]+', svm_line)\n",
    "    pred_labels_svm = [strip_prefix(t) for t in raw_pred_svm]\n",
    "\n",
    "    L = min(len(gt_labels), len(pred_labels))\n",
    "    if L == 0:\n",
    "        print(f\"Empty GT or prediction for {name}\")\n",
    "        continue\n",
    "\n",
    "    gt_seq = gt_labels[:L]\n",
    "    pred_seq = pred_labels[:L]\n",
    "    pred_seq_dt = pred_labels_dt[:L]\n",
    "    pred_seq_svm = pred_labels_svm[:L]\n",
    "\n",
    "    # 2a) Plate-level exact match for FINAL\n",
    "    plate_total += 1\n",
    "    if gt_seq == pred_seq and len(gt_labels) == len(pred_labels):\n",
    "        plate_exact_matches += 1\n",
    "\n",
    "    # 2b) Plate-level exact match for DT\n",
    "    if gt_seq == pred_seq_dt and len(gt_labels) == len(pred_labels_dt):\n",
    "        plate_exact_matches_dt += 1\n",
    "\n",
    "    # 2c) Plate-level exact match for SVM\n",
    "    if gt_seq == pred_seq_svm and len(gt_labels) == len(pred_labels_svm):\n",
    "        plate_exact_matches_svm += 1\n",
    "\n",
    "    # 2d) Sequence percentage for FINAL (original)\n",
    "    correct_chars = sum(1 for i in range(L) if gt_seq[i] == pred_seq[i])\n",
    "    seq_pct = correct_chars / len(gt_labels) * 100\n",
    "    sequence_scores.append(seq_pct)\n",
    "\n",
    "    # Accumulate for token-level metrics\n",
    "    all_gt.extend(gt_seq)\n",
    "    all_pred.extend(pred_seq)\n",
    "    all_pred_dt.extend(pred_seq_dt)\n",
    "    all_pred_svm.extend(pred_seq_svm)\n",
    "\n",
    "    print(f\"Plate {name}: exact_match={gt_seq==pred_seq}, sequence_pct={seq_pct:.1f}%\")\n",
    "\n",
    "# 3. Aggregate metrics\n",
    "plate_exact_acc = plate_exact_matches / plate_total * 100 if plate_total else 0\n",
    "plate_exact_acc_dt = plate_exact_matches_dt / plate_total * 100 if plate_total else 0\n",
    "plate_exact_acc_svm = plate_exact_matches_svm / plate_total * 100 if plate_total else 0\n",
    "avg_sequence_acc = np.mean(sequence_scores) if sequence_scores else 0\n",
    "token_acc = accuracy_score(all_gt, all_pred) * 100 if all_gt else 0\n",
    "token_acc_dt = accuracy_score(all_gt, all_pred_dt) * 100 if all_gt else 0\n",
    "token_acc_svm = accuracy_score(all_gt, all_pred_svm) * 100 if all_gt else 0\n",
    "\n",
    "# 4. Save & print\n",
    "with open(os.path.join(OUTPUT_DIR, 'metrics.txt'), 'w', encoding='utf-8') as f:\n",
    "    f.write(f\"Plates processed: {plate_total}\\n\")\n",
    "    f.write(f\"Plate exact-match accuracy (FINAL): {plate_exact_acc:.2f}%\\n\")\n",
    "    f.write(f\"Plate exact-match accuracy (DT):    {plate_exact_acc_dt:.2f}%\\n\")\n",
    "    f.write(f\"Plate exact-match accuracy (SVM):   {plate_exact_acc_svm:.2f}%\\n\")\n",
    "    f.write(f\"Average sequence accuracy (FINAL):  {avg_sequence_acc:.2f}%\\n\")\n",
    "    f.write(f\"Token-level accuracy (FINAL):       {token_acc:.2f}%\\n\")\n",
    "    f.write(f\"Token-level accuracy (DT):          {token_acc_dt:.2f}%\\n\")\n",
    "    f.write(f\"Token-level accuracy (SVM):         {token_acc_svm:.2f}%\\n\\n\")\n",
    "    if all_gt:\n",
    "        f.write(\"Classification Report (token-level, FINAL):\\n\")\n",
    "        f.write(classification_report(all_gt, all_pred, zero_division=0))\n",
    "\n",
    "print(\"\\nPhase 5 evaluation complete.\")\n",
    "print(f\"Plate exact-match (FINAL): {plate_exact_acc:.2f}%\")\n",
    "print(f\"Plate exact-match (DT):    {plate_exact_acc_dt:.2f}%\")\n",
    "print(f\"Plate exact-match (SVM):   {plate_exact_acc_svm:.2f}%\")\n",
    "print(f\"Avg sequence acc (FINAL):  {avg_sequence_acc:.2f}%\")\n",
    "print(f\"Token-level acc (FINAL):   {token_acc:.2f}%\")\n",
    "print(f\"Token-level acc (DT):      {token_acc_dt:.2f}%\")\n",
    "print(f\"Token-level acc (SVM):     {token_acc_svm:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02c1abf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plate 1: exact_match=False, sequence_pct=12.5%\n",
      "Plate 10: exact_match=False, sequence_pct=14.3%\n",
      "Plate 100: exact_match=False, sequence_pct=25.0%\n",
      "Plate 101: exact_match=False, sequence_pct=0.0%\n",
      "Plate 102: exact_match=False, sequence_pct=25.0%\n",
      "Plate 103: exact_match=False, sequence_pct=12.5%\n",
      "Plate 104: exact_match=False, sequence_pct=62.5%\n",
      "Plate 105: exact_match=False, sequence_pct=62.5%\n",
      "Plate 106: exact_match=False, sequence_pct=12.5%\n",
      "Plate 107: exact_match=False, sequence_pct=62.5%\n",
      "Plate 108: exact_match=False, sequence_pct=25.0%\n",
      "Plate 109: exact_match=False, sequence_pct=12.5%\n",
      "Plate 11: exact_match=False, sequence_pct=0.0%\n",
      "Plate 110: exact_match=False, sequence_pct=12.5%\n",
      "Plate 111: exact_match=False, sequence_pct=87.5%\n",
      "Plate 112: exact_match=False, sequence_pct=25.0%\n",
      "Plate 113: exact_match=False, sequence_pct=62.5%\n",
      "Plate 114: exact_match=False, sequence_pct=0.0%\n",
      "Plate 115: exact_match=False, sequence_pct=0.0%\n",
      "Plate 116: exact_match=False, sequence_pct=50.0%\n",
      "Plate 117: exact_match=False, sequence_pct=12.5%\n",
      "Plate 118: exact_match=False, sequence_pct=37.5%\n",
      "Plate 119: exact_match=False, sequence_pct=50.0%\n",
      "Plate 12: exact_match=False, sequence_pct=0.0%\n",
      "Plate 120: exact_match=False, sequence_pct=62.5%\n",
      "Plate 121: exact_match=False, sequence_pct=75.0%\n",
      "Plate 122: exact_match=False, sequence_pct=25.0%\n",
      "Plate 123: exact_match=False, sequence_pct=37.5%\n",
      "Plate 124: exact_match=False, sequence_pct=50.0%\n",
      "Plate 125: exact_match=False, sequence_pct=0.0%\n",
      "Plate 126: exact_match=False, sequence_pct=62.5%\n",
      "Plate 127: exact_match=False, sequence_pct=25.0%\n",
      "Plate 128: exact_match=False, sequence_pct=37.5%\n",
      "Plate 129: exact_match=False, sequence_pct=75.0%\n",
      "Plate 13: exact_match=False, sequence_pct=12.5%\n",
      "Plate 130: exact_match=False, sequence_pct=37.5%\n",
      "Plate 131: exact_match=False, sequence_pct=62.5%\n",
      "Plate 132: exact_match=False, sequence_pct=37.5%\n",
      "Plate 133: exact_match=False, sequence_pct=0.0%\n",
      "Plate 134: exact_match=False, sequence_pct=75.0%\n",
      "Plate 135: exact_match=False, sequence_pct=37.5%\n",
      "Plate 136: exact_match=False, sequence_pct=12.5%\n",
      "Plate 137: exact_match=False, sequence_pct=37.5%\n",
      "Plate 138: exact_match=False, sequence_pct=37.5%\n",
      "Plate 139: exact_match=False, sequence_pct=0.0%\n",
      "Plate 14: exact_match=False, sequence_pct=12.5%\n",
      "Plate 140: exact_match=False, sequence_pct=0.0%\n",
      "Plate 141: exact_match=False, sequence_pct=12.5%\n",
      "Plate 142: exact_match=False, sequence_pct=37.5%\n",
      "Missing result for 143\n",
      "Plate 144: exact_match=False, sequence_pct=12.5%\n",
      "Plate 145: exact_match=False, sequence_pct=75.0%\n",
      "Plate 146: exact_match=False, sequence_pct=0.0%\n",
      "Plate 147: exact_match=False, sequence_pct=0.0%\n",
      "Plate 149: exact_match=False, sequence_pct=25.0%\n",
      "Plate 15: exact_match=False, sequence_pct=12.5%\n",
      "Plate 150: exact_match=False, sequence_pct=25.0%\n",
      "Plate 151: exact_match=False, sequence_pct=25.0%\n",
      "Plate 152: exact_match=False, sequence_pct=25.0%\n",
      "Plate 153: exact_match=False, sequence_pct=12.5%\n",
      "Plate 154: exact_match=False, sequence_pct=12.5%\n",
      "Plate 155: exact_match=False, sequence_pct=0.0%\n",
      "Plate 156: exact_match=False, sequence_pct=37.5%\n",
      "Plate 157: exact_match=False, sequence_pct=12.5%\n",
      "Plate 158: exact_match=False, sequence_pct=62.5%\n",
      "Plate 159: exact_match=False, sequence_pct=0.0%\n",
      "Plate 16: exact_match=False, sequence_pct=12.5%\n",
      "Plate 160: exact_match=False, sequence_pct=12.5%\n",
      "Plate 161: exact_match=False, sequence_pct=62.5%\n",
      "Plate 162: exact_match=False, sequence_pct=0.0%\n",
      "Plate 163: exact_match=False, sequence_pct=12.5%\n",
      "Plate 164: exact_match=False, sequence_pct=12.5%\n",
      "Plate 165: exact_match=False, sequence_pct=0.0%\n",
      "Plate 166: exact_match=False, sequence_pct=25.0%\n",
      "Plate 167: exact_match=False, sequence_pct=37.5%\n",
      "Plate 168: exact_match=False, sequence_pct=0.0%\n",
      "Plate 169: exact_match=False, sequence_pct=12.5%\n",
      "Plate 17: exact_match=False, sequence_pct=0.0%\n",
      "Plate 170: exact_match=False, sequence_pct=37.5%\n",
      "Plate 171: exact_match=False, sequence_pct=0.0%\n",
      "Missing result for 172\n",
      "Plate 173: exact_match=False, sequence_pct=25.0%\n",
      "Plate 174: exact_match=False, sequence_pct=12.5%\n",
      "Plate 175: exact_match=False, sequence_pct=0.0%\n",
      "Plate 176: exact_match=False, sequence_pct=0.0%\n",
      "Plate 177: exact_match=False, sequence_pct=0.0%\n",
      "Plate 178: exact_match=False, sequence_pct=0.0%\n",
      "Plate 179: exact_match=False, sequence_pct=0.0%\n",
      "Plate 18: exact_match=False, sequence_pct=0.0%\n",
      "Plate 180: exact_match=False, sequence_pct=0.0%\n",
      "Plate 181: exact_match=False, sequence_pct=0.0%\n",
      "Plate 182: exact_match=False, sequence_pct=50.0%\n",
      "Plate 183: exact_match=False, sequence_pct=25.0%\n",
      "Plate 184: exact_match=False, sequence_pct=0.0%\n",
      "Plate 185: exact_match=False, sequence_pct=37.5%\n",
      "Plate 186: exact_match=False, sequence_pct=0.0%\n",
      "Plate 187: exact_match=False, sequence_pct=12.5%\n",
      "Plate 188: exact_match=False, sequence_pct=37.5%\n",
      "Plate 189: exact_match=False, sequence_pct=62.5%\n",
      "Plate 19: exact_match=False, sequence_pct=62.5%\n",
      "Plate 190: exact_match=False, sequence_pct=50.0%\n",
      "Plate 191: exact_match=False, sequence_pct=25.0%\n",
      "Plate 192: exact_match=False, sequence_pct=37.5%\n",
      "Plate 193: exact_match=False, sequence_pct=25.0%\n",
      "Plate 194: exact_match=False, sequence_pct=37.5%\n",
      "Plate 195: exact_match=False, sequence_pct=0.0%\n",
      "Plate 196: exact_match=False, sequence_pct=62.5%\n",
      "Plate 197: exact_match=False, sequence_pct=0.0%\n",
      "Plate 198: exact_match=False, sequence_pct=62.5%\n",
      "Plate 199: exact_match=False, sequence_pct=75.0%\n",
      "Plate 2: exact_match=False, sequence_pct=12.5%\n",
      "Plate 20: exact_match=False, sequence_pct=25.0%\n",
      "Plate 200: exact_match=False, sequence_pct=37.5%\n",
      "Plate 201: exact_match=False, sequence_pct=0.0%\n",
      "Plate 202: exact_match=False, sequence_pct=0.0%\n",
      "Plate 203: exact_match=False, sequence_pct=12.5%\n",
      "Plate 204: exact_match=False, sequence_pct=0.0%\n",
      "Plate 205: exact_match=False, sequence_pct=25.0%\n",
      "Plate 206: exact_match=False, sequence_pct=25.0%\n",
      "Plate 207: exact_match=False, sequence_pct=87.5%\n",
      "Plate 208: exact_match=False, sequence_pct=37.5%\n",
      "Plate 209: exact_match=False, sequence_pct=0.0%\n",
      "Plate 21: exact_match=False, sequence_pct=62.5%\n",
      "Plate 210: exact_match=False, sequence_pct=12.5%\n",
      "Plate 211: exact_match=False, sequence_pct=25.0%\n",
      "Plate 212: exact_match=False, sequence_pct=25.0%\n",
      "Plate 213: exact_match=False, sequence_pct=37.5%\n",
      "Plate 214: exact_match=False, sequence_pct=12.5%\n",
      "Plate 215: exact_match=False, sequence_pct=50.0%\n",
      "Plate 216: exact_match=False, sequence_pct=37.5%\n",
      "Plate 217: exact_match=False, sequence_pct=37.5%\n",
      "Plate 22: exact_match=False, sequence_pct=12.5%\n",
      "Plate 23: exact_match=False, sequence_pct=25.0%\n",
      "Plate 24: exact_match=False, sequence_pct=0.0%\n",
      "Plate 25: exact_match=False, sequence_pct=25.0%\n",
      "Plate 26: exact_match=False, sequence_pct=12.5%\n",
      "Plate 27: exact_match=False, sequence_pct=25.0%\n",
      "Plate 28: exact_match=False, sequence_pct=37.5%\n",
      "Plate 29: exact_match=False, sequence_pct=12.5%\n",
      "Plate 3: exact_match=False, sequence_pct=37.5%\n",
      "Plate 30: exact_match=False, sequence_pct=25.0%\n",
      "Plate 31: exact_match=False, sequence_pct=12.5%\n",
      "Plate 32: exact_match=False, sequence_pct=50.0%\n",
      "Plate 33: exact_match=False, sequence_pct=37.5%\n",
      "Plate 34: exact_match=False, sequence_pct=50.0%\n",
      "Plate 35: exact_match=False, sequence_pct=25.0%\n",
      "Plate 36: exact_match=False, sequence_pct=0.0%\n",
      "Plate 37: exact_match=False, sequence_pct=87.5%\n",
      "Plate 38: exact_match=False, sequence_pct=62.5%\n",
      "Plate 39: exact_match=False, sequence_pct=0.0%\n",
      "Plate 4: exact_match=False, sequence_pct=62.5%\n",
      "Plate 40: exact_match=False, sequence_pct=0.0%\n",
      "Plate 41: exact_match=False, sequence_pct=75.0%\n",
      "Plate 42: exact_match=False, sequence_pct=37.5%\n",
      "Plate 43: exact_match=False, sequence_pct=62.5%\n",
      "Plate 44: exact_match=False, sequence_pct=37.5%\n",
      "Plate 99: exact_match=False, sequence_pct=50.0%\n",
      "Plate 98: exact_match=False, sequence_pct=0.0%\n",
      "Plate 97: exact_match=False, sequence_pct=50.0%\n",
      "Plate 96: exact_match=False, sequence_pct=75.0%\n",
      "Plate 95: exact_match=False, sequence_pct=62.5%\n",
      "Plate 94: exact_match=False, sequence_pct=62.5%\n",
      "Plate 93: exact_match=False, sequence_pct=12.5%\n",
      "Plate 92: exact_match=False, sequence_pct=50.0%\n",
      "Plate 91: exact_match=False, sequence_pct=62.5%\n",
      "Plate 90: exact_match=False, sequence_pct=50.0%\n",
      "Plate 9: exact_match=False, sequence_pct=62.5%\n",
      "Plate 89: exact_match=False, sequence_pct=62.5%\n",
      "Plate 88: exact_match=False, sequence_pct=50.0%\n",
      "Plate 87: exact_match=False, sequence_pct=0.0%\n",
      "Plate 86: exact_match=False, sequence_pct=25.0%\n",
      "Plate 85: exact_match=False, sequence_pct=37.5%\n",
      "Plate 84: exact_match=False, sequence_pct=0.0%\n",
      "Plate 83: exact_match=False, sequence_pct=12.5%\n",
      "Plate 82: exact_match=False, sequence_pct=0.0%\n",
      "Plate 81: exact_match=False, sequence_pct=75.0%\n",
      "Plate 80: exact_match=False, sequence_pct=25.0%\n",
      "Plate 8: exact_match=False, sequence_pct=0.0%\n",
      "Plate 79: exact_match=False, sequence_pct=37.5%\n",
      "Plate 78: exact_match=False, sequence_pct=12.5%\n",
      "Plate 77: exact_match=False, sequence_pct=50.0%\n",
      "Plate 76: exact_match=False, sequence_pct=75.0%\n",
      "Plate 75: exact_match=False, sequence_pct=37.5%\n",
      "Plate 74: exact_match=False, sequence_pct=37.5%\n",
      "Plate 73: exact_match=False, sequence_pct=25.0%\n",
      "Plate 72: exact_match=False, sequence_pct=0.0%\n",
      "Plate 71: exact_match=False, sequence_pct=75.0%\n",
      "Plate 70: exact_match=False, sequence_pct=50.0%\n",
      "Plate 7: exact_match=False, sequence_pct=62.5%\n",
      "Plate 69: exact_match=False, sequence_pct=50.0%\n",
      "Plate 68: exact_match=False, sequence_pct=50.0%\n",
      "Plate 67: exact_match=False, sequence_pct=0.0%\n",
      "Plate 66: exact_match=False, sequence_pct=12.5%\n",
      "Plate 65: exact_match=False, sequence_pct=12.5%\n",
      "Plate 64: exact_match=False, sequence_pct=0.0%\n",
      "Plate 63: exact_match=False, sequence_pct=75.0%\n",
      "Plate 62: exact_match=False, sequence_pct=37.5%\n",
      "Plate 61: exact_match=False, sequence_pct=37.5%\n",
      "Plate 60: exact_match=True, sequence_pct=75.0%\n",
      "Plate 6: exact_match=False, sequence_pct=62.5%\n",
      "Plate 59: exact_match=False, sequence_pct=0.0%\n",
      "Plate 58: exact_match=False, sequence_pct=50.0%\n",
      "Plate 57: exact_match=False, sequence_pct=37.5%\n",
      "Plate 56: exact_match=False, sequence_pct=12.5%\n",
      "Plate 55: exact_match=False, sequence_pct=37.5%\n",
      "Plate 54: exact_match=False, sequence_pct=62.5%\n",
      "Plate 53: exact_match=False, sequence_pct=0.0%\n",
      "Plate 52: exact_match=False, sequence_pct=12.5%\n",
      "Plate 51: exact_match=False, sequence_pct=0.0%\n",
      "Plate 50: exact_match=False, sequence_pct=25.0%\n",
      "Plate 5: exact_match=False, sequence_pct=0.0%\n",
      "Plate 49: exact_match=False, sequence_pct=50.0%\n",
      "Plate 48: exact_match=False, sequence_pct=37.5%\n",
      "Plate 47: exact_match=False, sequence_pct=12.5%\n",
      "Missing result for 46\n",
      "Plate 45: exact_match=False, sequence_pct=62.5%\n",
      "\n",
      "Phase 5 evaluation complete.\n",
      "Plate exact-match: 0.00%\n",
      "Avg sequence acc:  29.41%\n",
      "Token-level acc:   33.22%\n"
     ]
    }
   ],
   "source": [
    "# Phase 5: Evaluate character recognition performance with sequence percentages\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Paths\n",
    "GROUND_TRUTH_CSV = r\"plate_labels.txt\"     # 'name,plate' where plate is e.g. \"6-jim,9-kh,…\"\n",
    "PREDICTION_DIR   = r\"plate_results\"        # contains <base>_plate_result.txt\n",
    "OUTPUT_DIR       = r\"evaluation_results\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def strip_prefix(token):\n",
    "    return token.split('-', 1)[-1]\n",
    "\n",
    "# 1. Load ground truth (strip numeric prefixes)\n",
    "gt_map = {}\n",
    "with open(GROUND_TRUTH_CSV, 'r', encoding='utf-8') as f:\n",
    "    lines = [ln.strip() for ln in f if ln.strip()]\n",
    "for ln in lines[1:]:\n",
    "    parts = ln.split(',')\n",
    "    if len(parts) < 2:\n",
    "        continue\n",
    "    name = parts[0].strip()\n",
    "    tail = ','.join(parts[1:]).strip()\n",
    "    raw_tokens = [t.strip() for t in tail.split(',') if t.strip()]\n",
    "    gt_map[name] = [strip_prefix(t) for t in raw_tokens]\n",
    "\n",
    "# 2. Iterate plates and compute metrics\n",
    "plate_total = 0\n",
    "plate_exact_matches = 0\n",
    "sequence_scores = []       # per-plate % correct in sequence\n",
    "all_gt, all_pred = [], []\n",
    "\n",
    "for name, gt_labels in gt_map.items():\n",
    "    base = name.split('_')[0]\n",
    "    res_file = os.path.join(PREDICTION_DIR, f\"{base}_plate_result.txt\")\n",
    "    if not os.path.isfile(res_file):\n",
    "        print(f\"Missing result for {name}\")\n",
    "        continue\n",
    "\n",
    "    with open(res_file, 'r', encoding='utf-8') as f:\n",
    "        lines = [ln.strip() for ln in f if ln.strip()]\n",
    "    final_line = next((ln for ln in lines if ln.startswith('FINAL:')), None)\n",
    "    if not final_line:\n",
    "        print(f\"No FINAL line in {res_file}\")\n",
    "        continue\n",
    "\n",
    "    raw_pred = re.findall(r'\\d+-[^\\d,]+', final_line)\n",
    "    pred_labels = [strip_prefix(t) for t in raw_pred]\n",
    "\n",
    "    L = min(len(gt_labels), len(pred_labels))\n",
    "    if L == 0:\n",
    "        print(f\"Empty GT or prediction for {name}\")\n",
    "        continue\n",
    "\n",
    "    gt_seq   = gt_labels[:L]\n",
    "    pred_seq = pred_labels[:L]\n",
    "\n",
    "    # 2a) plate‐level exact match\n",
    "    plate_total += 1\n",
    "    if gt_seq == pred_seq and len(gt_labels) == len(pred_labels):\n",
    "        plate_exact_matches += 1\n",
    "\n",
    "    # 2b) sequence percentage for this plate\n",
    "    correct_chars = sum(1 for i in range(L) if gt_seq[i] == pred_seq[i])\n",
    "    seq_pct = correct_chars / len(gt_labels) * 100\n",
    "    sequence_scores.append(seq_pct)\n",
    "\n",
    "    # accumulate for token‐level metrics\n",
    "    all_gt.extend(gt_seq)\n",
    "    all_pred.extend(pred_seq)\n",
    "\n",
    "    print(f\"Plate {name}: exact_match={gt_seq==pred_seq}, sequence_pct={seq_pct:.1f}%\")\n",
    "\n",
    "# 3. Aggregate\n",
    "plate_exact_acc = plate_exact_matches / plate_total * 100 if plate_total else 0\n",
    "avg_sequence_acc = np.mean(sequence_scores) if sequence_scores else 0\n",
    "token_acc = accuracy_score(all_gt, all_pred) * 100 if all_gt else 0\n",
    "\n",
    "# 4. Save & print\n",
    "with open(os.path.join(OUTPUT_DIR, 'metrics.txt'), 'w', encoding='utf-8') as f:\n",
    "    f.write(f\"Plates processed: {plate_total}\\n\")\n",
    "    f.write(f\"Plate exact-match accuracy: {plate_exact_acc:.2f}%\\n\")\n",
    "    f.write(f\"Average sequence accuracy:   {avg_sequence_acc:.2f}%\\n\")\n",
    "    f.write(f\"Token-level accuracy:        {token_acc:.2f}%\\n\\n\")\n",
    "    if all_gt:\n",
    "        f.write(\"Classification Report (token-level):\\n\")\n",
    "        f.write(classification_report(all_gt, all_pred, zero_division=0))\n",
    "\n",
    "print(\"\\nPhase 5 evaluation complete.\")\n",
    "print(f\"Plate exact-match: {plate_exact_acc:.2f}%\")\n",
    "print(f\"Avg sequence acc:  {avg_sequence_acc:.2f}%\")\n",
    "print(f\"Token-level acc:   {token_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351895f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# if all_gt:\n",
    "#     cm_display = ConfusionMatrixDisplay.from_predictions(all_gt, all_pred, xticks_rotation=90)\n",
    "#     plt.title(\"Confusion Matrix (Token Level)\")\n",
    "#     plt.savefig(os.path.join(OUTPUT_DIR, \"confusion_matrix.png\"))\n",
    "#     plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
